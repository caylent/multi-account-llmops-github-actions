{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267f9bde-a95f-4ffd-a471-f2d082d8b097",
   "metadata": {},
   "source": [
    "### This end-to-end training notebook contains code to fine-tune the Multilingual Cased DistilBERT model with or without hyperparameter optimization on our training dataset and create a model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab61a0d-c731-422a-9291-09575e346f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#Run the utility notebook first\n",
    "%run distilBERT_utility.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9cdb4-ed3e-4a44-b21b-c286f850bb03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Starting a SageMaker session with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3aeba7-ceea-4126-b430-7158da51466b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484bbb4-7d00-4512-8eb0-460fd29c9c26",
   "metadata": {},
   "source": [
    "### Setting Model ID and version for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9b0903-4035-4fe8-bfc4-d8b56a7835e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = 'huggingface-tc-distilbert-base-multilingual-cased', \"2.0.0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd65bc6-014c-4c11-acae-7909fdc9022c",
   "metadata": {},
   "source": [
    "### Set training data and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9487f8b-fa2e-4e34-a750-32774e1506de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset_s3_path = f\"s3://sagemaker-sigparser-caylent-mlops/data/email-type/input/processed/distilbert/distilbert-train/distilbert-fine_tuning-24-04-2024/data.csv\"\n",
    "s3_output_location = f\"s3://sagemaker-sigparser-caylent-mlops/model/email-type/distilBERT/{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64190fab-18e1-444d-956f-9afeb9c526cd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048f4524-2096-47bc-94e8-43c978e73b37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-tc-distilbert-base-multilingual-cased' with wildcard version identifier '*'. You can pin to version '2.0.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {'epochs': '4',\n",
    " 'learning_rate': '2e-05',\n",
    " 'batch_size': '64',\n",
    " 'eval_batch_size': '8',\n",
    " 'eval_accumulation_steps': 'None',\n",
    " 'reinitialize_top_layer': 'Auto',\n",
    " 'train_only_top_layer': 'False'}\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    output_path=s3_output_location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cca66cf5-0cff-40d3-9144-2e36a88522f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: distilbert-model-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using estimator\n",
      "2024-04-26 23:06:59 Starting - Starting the training job\n",
      "2024-04-26 23:06:59 Pending - Training job waiting for capacity......\n",
      "2024-04-26 23:07:38 Pending - Preparing the instances for training...\n",
      "2024-04-26 23:08:12 Downloading - Downloading input data...\n",
      "2024-04-26 23:08:43 Downloading - Downloading the training image........................\n",
      "2024-04-26 23:12:59 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-26 23:13:21,827 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-26 23:13:21,854 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:13:21,856 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:13:21,864 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-prepack-script-utilities\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-jumpstart-prepack-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: There was an error checking the latest version of pip.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:13:24,033 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:13:24,033 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:13:24,133 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": \"64\",\n",
      "        \"epochs\": \"4\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_batch_size\": \"8\",\n",
      "        \"learning_rate\": \"2e-05\",\n",
      "        \"reinitialize_top_layer\": \"Auto\",\n",
      "        \"train_only_top_layer\": \"False\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"distilbert-model-1\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":\"64\",\"epochs\":\"4\",\"eval_accumulation_steps\":\"None\",\"eval_batch_size\":\"8\",\"learning_rate\":\"2e-05\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"False\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":\"64\",\"epochs\":\"4\",\"eval_accumulation_steps\":\"None\",\"eval_batch_size\":\"8\",\"learning_rate\":\"2e-05\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"False\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"distilbert-model-1\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--epochs\",\"4\",\"--eval_accumulation_steps\",\"None\",\"--eval_batch_size\",\"8\",\"--learning_rate\",\"2e-05\",\"--reinitialize_top_layer\",\"Auto\",\"--train_only_top_layer\",\"False\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_REINITIALIZE_TOP_LAYER=Auto\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_ONLY_TOP_LAYER=False\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --batch_size 64 --epochs 4 --eval_accumulation_steps None --eval_batch_size 8 --learning_rate 2e-05 --reinitialize_top_layer Auto --train_only_top_layer False\u001b[0m\n",
      "\u001b[34mRunning training scripts with arguments: Namespace(batch_size=64, current_host='algo-1', epochs=4, eval_accumulation_steps='None', eval_batch_size=8, hosts=['algo-1'], hub_key=None, learning_rate=2e-05, logging_frequency=50, metric_for_best_model='f1', model_dir='/opt/ml/model', pretrained_model='/opt/ml/input/data/model', reinitialize_top_layer='Auto', seed=0, train='/opt/ml/input/data/training', train_only_top_layer='False').\u001b[0m\n",
      "\u001b[34mIgnoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-9c0b383fbe01cde1\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-9c0b383fbe01cde1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 6069.90it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 1226.76it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-9c0b383fbe01cde1/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 841.89it/s]\u001b[0m\n",
      "\u001b[34mloaded dataset sizes is: 3397\u001b[0m\n",
      "\u001b[34mThe labels distribution in the dataset is: {1: 2355, 0: 1042}\u001b[0m\n",
      "\u001b[34mParameter 'function'=<function _prepare_data.<locals>.<lambda> at 0x7fbee8019040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m202ex [00:00, 1998.40ex/s]\u001b[0m\n",
      "\u001b[34m402ex [00:00, 1856.21ex/s]\u001b[0m\n",
      "\u001b[34m753ex [00:00, 2578.94ex/s]\u001b[0m\n",
      "\u001b[34m1082ex [00:00, 2850.45ex/s]\u001b[0m\n",
      "\u001b[34m1504ex [00:00, 3332.80ex/s]\u001b[0m\n",
      "\u001b[34m1916ex [00:00, 3594.31ex/s]\u001b[0m\n",
      "\u001b[34m2299ex [00:00, 3669.88ex/s]\u001b[0m\n",
      "\u001b[34m2712ex [00:00, 3815.33ex/s]\u001b[0m\n",
      "\u001b[34m3095ex [00:00, 3789.91ex/s]\u001b[0m\n",
      "\u001b[34m3397ex [00:00, 3419.44ex/s]\u001b[0m\n",
      "\u001b[34mloaded train_dataset sizes is: 2717\u001b[0m\n",
      "\u001b[34mThe labels distribution in the train dataset is: {1: 1876, 0: 841}\u001b[0m\n",
      "\u001b[34mloaded eval_dataset sizes is: 680\u001b[0m\n",
      "\u001b[34mThe labels distribution in the eval dataset is: {1: 479, 0: 201}\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 2717\u001b[0m\n",
      "\u001b[34mNum examples = 2717\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34mNum Epochs = 4\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 172\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 172\u001b[0m\n",
      "\u001b[34m[2024-04-26 23:13:41.629 algo-1:46 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-26 23:13:41.794 algo-1:46 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-04-26 23:13:41.796 algo-1:46 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-26 23:13:41.796 algo-1:46 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-04-26 23:13:41.797 algo-1:46 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-26 23:13:41.797 algo-1:46 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m{'loss': 0.6621, 'learning_rate': 1.988372093023256e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.13454914093017578, 'eval_accuracy': 0.9558823529411765, 'eval_f1': 0.956508437917983, 'eval_precision': 0.9593712680095148, 'eval_recall': 0.9558823529411765, 'eval_runtime': 0.9614, 'eval_samples_per_second': 707.282, 'eval_steps_per_second': 88.41, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-43\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-43\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-43/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-43/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-43/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-43/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-43/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-43/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-43/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-43/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.3074, 'learning_rate': 1.4186046511627909e-05, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 680\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.0799405574798584, 'eval_accuracy': 0.975, 'eval_f1': 0.9750533255768857, 'eval_precision': 0.975151384083045, 'eval_recall': 0.975, 'eval_runtime': 0.9471, 'eval_samples_per_second': 717.978, 'eval_steps_per_second': 89.747, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-86\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-86\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-86/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-86/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-86/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-86/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-86/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-86/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-86/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-86/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-43] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-43] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0753, 'learning_rate': 8.372093023255815e-06, 'epoch': 2.33}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.07876692712306976, 'eval_accuracy': 0.9764705882352941, 'eval_f1': 0.976436274509804, 'eval_precision': 0.976422162174453, 'eval_recall': 0.9764705882352941, 'eval_runtime': 0.928, 'eval_samples_per_second': 732.757, 'eval_steps_per_second': 91.595, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-129\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-129\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-129/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-129/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-129/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-129/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-129/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-129/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-129/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-129/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-86] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-86] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.032, 'learning_rate': 2.558139534883721e-06, 'epoch': 3.49}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.08996591717004776, 'eval_accuracy': 0.9735294117647059, 'eval_f1': 0.9737110439604132, 'eval_precision': 0.9743797342095926, 'eval_recall': 0.9735294117647059, 'eval_runtime': 0.9286, 'eval_samples_per_second': 732.261, 'eval_steps_per_second': 91.533, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-172\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-172\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-172/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-172/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-172/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-172/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-172/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-172/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-172/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-172/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-129 (score: 0.976436274509804).\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-129 (score: 0.976436274509804).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 48.1959, 'train_samples_per_second': 225.496, 'train_steps_per_second': 3.569, 'train_loss': 0.1259420167568118, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mInfo file not found at '/tmp/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:14:31,648 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:14:31,648 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-26 23:14:31,648 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-26 23:14:56 Uploading - Uploading generated training model\n",
      "2024-04-26 23:14:56 Completed - Training job completed\n",
      "Training seconds: 403\n",
      "Billable seconds: 403\n"
     ]
    }
   ],
   "source": [
    "#For Hyperparameter Optimization, set use_amt to True\n",
    "use_amt = False \n",
    "\n",
    "#For HPO:Select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"val_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9\\\\.]+)\"}],\n",
    "    \"type\": \"Maximize\",\n",
    "}\n",
    "\n",
    "#If use_amt is set to True, a hyperparameter optimization job is carried out.\n",
    "if use_amt:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        estimator, \n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=6,\n",
    "        max_parallel_jobs=2,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=\"distilbert-multilingual-\",  \n",
    "    )\n",
    "\n",
    "    #Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"training\": training_dataset_s3_path})\n",
    "    \n",
    "#Else, a regular training job is created\n",
    "else:\n",
    "    training_job_name = 'distilbert-model-1' #change this value to change the training job's name and folder name where the model artifacts are stored\n",
    "    #Note that the training job's name must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "    print(\"using estimator\")\n",
    "    estimator.fit({\"training\": training_dataset_s3_path}, logs=True, job_name=training_job_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0fbdd307-6e2b-4718-9dcd-7ead5eccb570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>hugginface-tc:eval-accuracy</td>\n",
       "      <td>0.955882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.0</td>\n",
       "      <td>hugginface-tc:eval-accuracy</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                  metric_name     value\n",
       "0        0.0  hugginface-tc:eval-accuracy  0.955882\n",
       "1       60.0  hugginface-tc:eval-accuracy  0.975000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training performance metrics\n",
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "    model_data = hp_tuner.best_estimator().model_data\n",
    "else:\n",
    "    training_job_name = estimator.latest_training_job.job_name\n",
    "    model_data = estimator.model_data\n",
    "    \n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010bcf1-b001-42d3-8852-9e8c8d24adf5",
   "metadata": {},
   "source": [
    "### Refactoring compressed model artifacts and re-routing S3 path (for deployment purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d3494b4-62cf-4b68-af72-7c7c1d9b4cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#You can either derive model_data from the above cell after training, or hard-code it as done below .\n",
    "model_data = 's3://sagemaker-sigparser-caylent-mlops/model/email-type/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model.tar.gz'\n",
    "source_bucket = model_data[5:].split('/', 1)[0] #Bucket name\n",
    "\n",
    "#We will extract the following string 'model/email-type/distilBERT/2024-04-26_23-06-31/distilbert-model-1' from model_data path given in above example\n",
    "source_prefix = model_data.split('sagemaker-sigparser-caylent-mlops/')[1].rsplit('/', 2)[0] \n",
    "\n",
    "#Next, we will extract just the timestamp and training job name from model_data path to create the destination path\n",
    "destination_inter_prefix = model_data.split('distilBERT/')[1].split('/output')[0]\n",
    "destination_prefix = f'model-artifacts/distilBERT/{destination_inter_prefix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5ce3ffe-c591-4a1a-8b96-13556f79e6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/debug-output/training_job_end.ts\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/config.json\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/special_tokens_map.json\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/vocab.txt\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/code/inference.py\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/code/constants/constants.py\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/code/constants/__init__.py\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/code/version\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/code/__init__.py\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/code/__script_info__.json\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/__models_info__.json\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/pytorch_model.bin\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/tokenizer_config.json\n",
      "Copied file to model-artifacts/distilBERT/2024-04-26_23-06-31/distilbert-model-1/output/model/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "for result in paginator.paginate(Bucket=source_bucket, Prefix=source_prefix):\n",
    "    if 'Contents' in result:\n",
    "        #List all objects in the source directory containing all three output folders after training\n",
    "        for obj in result['Contents']:\n",
    "            key = obj['Key']\n",
    "            #Get folder names\n",
    "            folder_name = key.split('/')[-2]\n",
    "            #The 'output' folder contains the model.tar.gz file\n",
    "            if folder_name == 'output' and key.endswith('model.tar.gz'):\n",
    "                #Uncompress and upload model.tar.gz files to 'output/model' path\n",
    "                obj = s3_client.get_object(Bucket=source_bucket, Key=key)\n",
    "                with tarfile.open(fileobj=BytesIO(obj['Body'].read()), mode=\"r:gz\") as tar: #Extracting from compressed tar.gz file\n",
    "                    for member in tar.getmembers():\n",
    "                        if member.isfile():\n",
    "                            #Extracted files are stored in path output/model in the folder\n",
    "                            extracted_file_key = os.path.join(destination_prefix, 'output', 'model', os.path.normpath(member.name))\n",
    "                            s3_client.upload_fileobj(BytesIO(tar.extractfile(member).read()), source_bucket, extracted_file_key)\n",
    "                            print(\"Copied file to\", extracted_file_key)\n",
    "                            \n",
    "            elif folder_name in ['debug-output', 'profiler-output']:\n",
    "                #Copy other folders and files as they are. It will skip copying any files which are 0 bytes.\n",
    "                new_key = key.replace(source_prefix, destination_prefix, 1)  \n",
    "                s3_client.copy_object(\n",
    "                    Bucket=source_bucket,\n",
    "                    CopySource={'Bucket': source_bucket, 'Key': key},\n",
    "                    Key=new_key\n",
    "                )\n",
    "                print(\"Copied file to\", new_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125b336-267a-4b8f-80ef-53fdeaf851d7",
   "metadata": {},
   "source": [
    "### Incrementally train the fine-tuned model\n",
    "\n",
    "You can use the artifacts from an existing model and use an expanded dataset to train a new model as long as the dataset format remain the same (set of classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a32afd-ae07-4315-89c0-d52e585652dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Identify the previously trained training job name.\n",
    "last_training_job_name = 'distilbert-model-1'\n",
    "\n",
    "#Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "#Retrieve all details about the previous training job. Several items will be picked up from the resulting JSON object to create the estimator in the code block below.\n",
    "training_job_info = sagemaker_client.describe_training_job(TrainingJobName=last_training_job_name)\n",
    "\n",
    "#New training dataset's S3 path\n",
    "training_dataset_s3_path = 's3://sagemaker-sigparser-caylent-mlops/data/email-type/input/processed/distilbert/distilbert-train/distilbert-fine_tuning-24-04-2024/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0c9cca0-e71d-4659-8431-0da61d899f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: distilbert-model-1-incremental-training-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-30 22:23:02 Starting - Starting the training job...\n",
      "2024-04-30 22:23:18 Starting - Preparing the instances for training...\n",
      "2024-04-30 22:23:49 Downloading - Downloading input data...\n",
      "2024-04-30 22:24:29 Downloading - Downloading the training image...........................\n",
      "2024-04-30 22:28:40 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-30 22:28:52,691 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-30 22:28:52,716 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-30 22:28:52,720 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-30 22:28:52,951 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-prepack-script-utilities\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-jumpstart-prepack-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-04-30 22:28:54,807 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-30 22:28:54,807 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-30 22:28:54,910 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": \"64\",\n",
      "        \"epochs\": \"4\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_batch_size\": \"8\",\n",
      "        \"learning_rate\": \"2e-05\",\n",
      "        \"reinitialize_top_layer\": \"Auto\",\n",
      "        \"train_only_top_layer\": \"False\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"distilbert-model-1-incremental-training-0\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/tc/prepack/v1.1.0/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":\"64\",\"epochs\":\"4\",\"eval_accumulation_steps\":\"None\",\"eval_batch_size\":\"8\",\"learning_rate\":\"2e-05\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"False\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/tc/prepack/v1.1.0/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":\"64\",\"epochs\":\"4\",\"eval_accumulation_steps\":\"None\",\"eval_batch_size\":\"8\",\"learning_rate\":\"2e-05\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"False\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"distilbert-model-1-incremental-training-0\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/tc/prepack/v1.1.0/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--epochs\",\"4\",\"--eval_accumulation_steps\",\"None\",\"--eval_batch_size\",\"8\",\"--learning_rate\",\"2e-05\",\"--reinitialize_top_layer\",\"Auto\",\"--train_only_top_layer\",\"False\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_REINITIALIZE_TOP_LAYER=Auto\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_ONLY_TOP_LAYER=False\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --batch_size 64 --epochs 4 --eval_accumulation_steps None --eval_batch_size 8 --learning_rate 2e-05 --reinitialize_top_layer Auto --train_only_top_layer False\u001b[0m\n",
      "\u001b[34mRunning training scripts with arguments: Namespace(batch_size=64, current_host='algo-1', epochs=4, eval_accumulation_steps='None', eval_batch_size=8, hosts=['algo-1'], hub_key=None, learning_rate=2e-05, logging_frequency=50, metric_for_best_model='f1', model_dir='/opt/ml/model', pretrained_model='/opt/ml/input/data/model', reinitialize_top_layer='Auto', seed=0, train='/opt/ml/input/data/training', train_only_top_layer='False').\u001b[0m\n",
      "\u001b[34mIgnoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-3fb0913e812f1ee4\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-3fb0913e812f1ee4/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 6432.98it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 1496.90it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3fb0913e812f1ee4/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 989.92it/s]\u001b[0m\n",
      "\u001b[34mloaded dataset sizes is: 3397\u001b[0m\n",
      "\u001b[34mThe labels distribution in the dataset is: {1: 2355, 0: 1042}\u001b[0m\n",
      "\u001b[34mParameter 'function'=<function _prepare_data.<locals>.<lambda> at 0x7fb8481341f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m299ex [00:00, 2987.74ex/s]\u001b[0m\n",
      "\u001b[34m598ex [00:00, 2473.97ex/s]\u001b[0m\n",
      "\u001b[34m1029ex [00:00, 3225.92ex/s]\u001b[0m\n",
      "\u001b[34m1565ex [00:00, 4012.89ex/s]\u001b[0m\n",
      "\u001b[34m2083ex [00:00, 4415.62ex/s]\u001b[0m\n",
      "\u001b[34m2642ex [00:00, 4803.37ex/s]\u001b[0m\n",
      "\u001b[34m3145ex [00:00, 4873.15ex/s]\u001b[0m\n",
      "\u001b[34m3397ex [00:00, 4331.80ex/s]\u001b[0m\n",
      "\u001b[34mloaded train_dataset sizes is: 2717\u001b[0m\n",
      "\u001b[34mThe labels distribution in the train dataset is: {1: 1876, 0: 841}\u001b[0m\n",
      "\u001b[34mloaded eval_dataset sizes is: 680\u001b[0m\n",
      "\u001b[34mThe labels distribution in the eval dataset is: {1: 479, 0: 201}\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 2717\n",
      "  Num Epochs = 4\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 2717\n",
      "  Num Epochs = 4\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 172\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 172\u001b[0m\n",
      "\u001b[34m[2024-04-30 22:29:11.257 algo-1:44 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-30 22:29:11.403 algo-1:44 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-04-30 22:29:11.405 algo-1:44 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-30 22:29:11.405 algo-1:44 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-04-30 22:29:11.406 algo-1:44 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-30 22:29:11.406 algo-1:44 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m{'loss': 0.0218, 'learning_rate': 1.988372093023256e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.07933121919631958, 'eval_accuracy': 0.9808823529411764, 'eval_f1': 0.9808960837849144, 'eval_precision': 0.9809148546530754, 'eval_recall': 0.9808823529411764, 'eval_runtime': 1.0548, 'eval_samples_per_second': 644.693, 'eval_steps_per_second': 80.587, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-43\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-43\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-43/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-43/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-43/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-43/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-43/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-43/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-43/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-43/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.0378, 'learning_rate': 1.4186046511627909e-05, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.10020023584365845, 'eval_accuracy': 0.975, 'eval_f1': 0.9751877344283337, 'eval_precision': 0.9759645621673924, 'eval_recall': 0.975, 'eval_runtime': 1.1016, 'eval_samples_per_second': 617.304, 'eval_steps_per_second': 77.163, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-86\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-86\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-86/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-86/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-86/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-86/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-86/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-86/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-86/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-86/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.026, 'learning_rate': 8.372093023255815e-06, 'epoch': 2.33}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.08451137691736221, 'eval_accuracy': 0.9794117647058823, 'eval_f1': 0.979441189873356, 'eval_precision': 0.9794906772077732, 'eval_recall': 0.9794117647058823, 'eval_runtime': 1.109, 'eval_samples_per_second': 613.158, 'eval_steps_per_second': 76.645, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-129\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-129\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-129/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-129/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-129/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-129/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-129/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-129/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-129/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-129/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-86] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-86] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0085, 'learning_rate': 2.558139534883721e-06, 'epoch': 3.49}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.08735805749893188, 'eval_accuracy': 0.9779411764705882, 'eval_f1': 0.9779882284501933, 'eval_precision': 0.978080202669303, 'eval_recall': 0.9779411764705882, 'eval_runtime': 1.114, 'eval_samples_per_second': 610.389, 'eval_steps_per_second': 76.299, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-172\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-172\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-172/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-172/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-172/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-172/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-172/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-172/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-172/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-172/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-129] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-129] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-43 (score: 0.9808960837849144).\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-43 (score: 0.9808960837849144).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 95.2621, 'train_samples_per_second': 114.085, 'train_steps_per_second': 1.806, 'train_loss': 0.02128810567651377, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2024-04-30 22:30:48,759 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-30 22:30:48,759 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-30 22:30:48,760 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-30 22:30:56 Uploading - Uploading generated training model\n",
      "2024-04-30 22:31:17 Completed - Training job completed\n",
      "Training seconds: 447\n",
      "Billable seconds: 447\n"
     ]
    }
   ],
   "source": [
    "#Set a name for the new incremental training job\n",
    "incremental_training_job_name = f\"{last_training_job_name}-incremental-training-0\"\n",
    "training_instance_type = \"ml.g4dn.xlarge\" #Set instance type for the new training job\n",
    "\n",
    "#Creating the new estimator job\n",
    "incremental_train_estimator = Estimator(\n",
    "    model_id=model_id,\n",
    "    role=aws_role,\n",
    "    image_uri=training_job_info['AlgorithmSpecification']['TrainingImage'], #Previous training job's image uri\n",
    "    model_uri=training_job_info['ModelArtifacts']['S3ModelArtifacts'], #Trained model's S3 uri (tar.gz file)\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    source_dir=training_job_info['InputDataConfig'][2]['DataSource']['S3DataSource']['S3Uri'], #Source directory containing transfer_learning.py\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters, #Already set in the beginning of this notebook\n",
    "    output_path=s3_output_location, #Already set in the beginning of this notebook, can be changed here if needed\n",
    "    #base_job_name=incremental_training_job_name, #Commenting this out since the job_name is fed while using estimator.fit below\n",
    "    metric_definitions=training_job_info['AlgorithmSpecification']['MetricDefinitions'] #Training metric definitions picked up from the previous training job's JSON object\n",
    ")\n",
    "\n",
    "#Fitting the above created estimator object to the new training data to create a new model\n",
    "incremental_train_estimator.fit({\"training\": training_dataset_s3_path}, logs=True, job_name=incremental_training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c96cb-9e5b-4e9a-b61a-93e6d98bf1cd",
   "metadata": {},
   "source": [
    "##### The model files for the above training job are stored in 's3://sagemaker-sigparser-caylent-mlops/model/email-type/distilBERT/2024-04-30_21-42-06/distilbert-model-1-incremental-training-0/'"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
