{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37709149-31f9-4d02-9d80-93ac34c02f8a",
   "metadata": {},
   "source": [
    "## Email Name: Prepare Training Data & Finetune Mistral-7B\n",
    "\n",
    "In this notebook, we will prepare the training data and perform instruction fine-tuning the `Mistral-7B` LLM.\n",
    "\n",
    "- Training data is formatted in JSON lines (`.jsonl`) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple `.jsonl` files. \n",
    "- The training folder can also contain a `template.json` file describing the input and output formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da68f89-7028-42a2-b38c-1adabe2d190d",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310d123f-4154-4ab0-9cdf-ed7c376fac5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::818442660361:role/service-role/AmazonSageMaker-ExecutionRole-20231103T203000\n",
      "sagemaker session region: us-east-1\n",
      "sagemaker default bucket: sagemaker-sigparser-caylent-mlops\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket='sagemaker-sigparser-caylent-mlops'\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker default bucket: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bf71f3-8bb3-4823-8cbd-c448fadd211f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import boto3\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "from utils.s3_helper import read_s3_csv_to_dataframe\n",
    "from prompts.email_names_v2 import prompt_email_names\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import json\n",
    "import datetime\n",
    "from sagemaker import hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c5229-ec03-418d-82d2-05dac8e22d0b",
   "metadata": {},
   "source": [
    "### Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2cd9e11-58b8-4c84-b1d6-6875a6bbcf51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_input_prefix = 'data/email-names/input/raw'\n",
    "data_timestamp = '2024-04-08'\n",
    "file_name = f'sp_llm_emailname_training-apr8.csv'\n",
    "\n",
    "s3_data_path = f\"{s3_input_prefix}/{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2736ae2f-3070-4159-a7b0-b143a405fd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21150, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = read_s3_csv_to_dataframe(sess.default_bucket(), s3_data_path)\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9730433-ff98-4ae9-b468-7b7cdfacef02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Group</th>\n",
       "      <th>Email Address</th>\n",
       "      <th>Display Name</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Middle Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Name Prefix</th>\n",
       "      <th>Name Suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_anderson@allianca.com</td>\n",
       "      <td>Alex Anderson</td>\n",
       "      <td>Alex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_anderson@alliant.com</td>\n",
       "      <td>Alex - Alliant Insurance Ltd. Anderson</td>\n",
       "      <td>Alex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_bell@carpenterfarraday.com</td>\n",
       "      <td>ANNA L. BELL</td>\n",
       "      <td>Anna</td>\n",
       "      <td>L.</td>\n",
       "      <td>Bell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_bodnar@orquest.com</td>\n",
       "      <td>BODNAR, Akshay (external)</td>\n",
       "      <td>Akshay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bodnar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_brown@onesteamboatplace.com</td>\n",
       "      <td>BROWN Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Test Group                  Email Address  \\\n",
       "0   TRAINING        a_anderson@allianca.com   \n",
       "1   TRAINING         a_anderson@alliant.com   \n",
       "2   TRAINING   a_bell@carpenterfarraday.com   \n",
       "3   TRAINING           a_bodnar@orquest.com   \n",
       "4   TRAINING  a_brown@onesteamboatplace.com   \n",
       "\n",
       "                             Display Name First Name Middle Name Last Name  \\\n",
       "0                           Alex Anderson       Alex         NaN  Anderson   \n",
       "1  Alex - Alliant Insurance Ltd. Anderson       Alex         NaN  Anderson   \n",
       "2                            ANNA L. BELL       Anna          L.      Bell   \n",
       "3               BODNAR, Akshay (external)     Akshay         NaN    Bodnar   \n",
       "4                              BROWN Adam       Adam         NaN     Brown   \n",
       "\n",
       "  Name Prefix Name Suffix  \n",
       "0         NaN         NaN  \n",
       "1         NaN         NaN  \n",
       "2         NaN         NaN  \n",
       "3         NaN         NaN  \n",
       "4         NaN         NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d4703-46e8-4d0c-a07d-46b14a330846",
   "metadata": {},
   "source": [
    "### Train Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae2825d7-06f4-4f37-a9eb-5b726c8d4757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count per column: Test Group           0\n",
      "Email Address        0\n",
      "Display Name         0\n",
      "First Name           1\n",
      "Middle Name      18884\n",
      "Last Name            0\n",
      "Name Prefix      21039\n",
      "Name Suffix      21017\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# finding how many NaN values are there in each column\n",
    "nan_count_per_column = train_df.isna().sum()\n",
    "\n",
    "print(f\"NaN count per column: {nan_count_per_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15dff6d3-3b68-4154-835c-8bec3fda9977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count per column after replacement: Test Group       0\n",
      "Email Address    0\n",
      "Display Name     0\n",
      "First Name       0\n",
      "Middle Name      0\n",
      "Last Name        0\n",
      "Name Prefix      0\n",
      "Name Suffix      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# fill NaN values with empty string\n",
    "train_df.fillna(\"\", inplace=True)\n",
    "\n",
    "print(f\"NaN count per column after replacement: {train_df.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d4ac0bc-0b33-464c-aa28-d4c038beaa80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Group</th>\n",
       "      <th>Email Address</th>\n",
       "      <th>Display Name</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Middle Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Name Prefix</th>\n",
       "      <th>Name Suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_anderson@allianca.com</td>\n",
       "      <td>Alex Anderson</td>\n",
       "      <td>Alex</td>\n",
       "      <td></td>\n",
       "      <td>Anderson</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_anderson@alliant.com</td>\n",
       "      <td>Alex - Alliant Insurance Ltd. Anderson</td>\n",
       "      <td>Alex</td>\n",
       "      <td></td>\n",
       "      <td>Anderson</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_bell@carpenterfarraday.com</td>\n",
       "      <td>ANNA L. BELL</td>\n",
       "      <td>Anna</td>\n",
       "      <td>L.</td>\n",
       "      <td>Bell</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_bodnar@orquest.com</td>\n",
       "      <td>BODNAR, Akshay (external)</td>\n",
       "      <td>Akshay</td>\n",
       "      <td></td>\n",
       "      <td>Bodnar</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>a_brown@onesteamboatplace.com</td>\n",
       "      <td>BROWN Adam</td>\n",
       "      <td>Adam</td>\n",
       "      <td></td>\n",
       "      <td>Brown</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Test Group                  Email Address  \\\n",
       "0   TRAINING        a_anderson@allianca.com   \n",
       "1   TRAINING         a_anderson@alliant.com   \n",
       "2   TRAINING   a_bell@carpenterfarraday.com   \n",
       "3   TRAINING           a_bodnar@orquest.com   \n",
       "4   TRAINING  a_brown@onesteamboatplace.com   \n",
       "\n",
       "                             Display Name First Name Middle Name Last Name  \\\n",
       "0                           Alex Anderson       Alex              Anderson   \n",
       "1  Alex - Alliant Insurance Ltd. Anderson       Alex              Anderson   \n",
       "2                            ANNA L. BELL       Anna          L.      Bell   \n",
       "3               BODNAR, Akshay (external)     Akshay                Bodnar   \n",
       "4                              BROWN Adam       Adam                 Brown   \n",
       "\n",
       "  Name Prefix Name Suffix  \n",
       "0                          \n",
       "1                          \n",
       "2                          \n",
       "3                          \n",
       "4                          "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30e9c98b-14d6-4ba9-830b-6ded35111099",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution (%) of empty strings in specified columns:\n",
      "First Name      0.004728\n",
      "Middle Name    89.286052\n",
      "Last Name       0.000000\n",
      "Name Prefix    99.475177\n",
      "Name Suffix    99.371158\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# cols to check for empty string distribution\n",
    "columns_to_check = ['First Name', 'Middle Name', 'Last Name', 'Name Prefix', 'Name Suffix']\n",
    "\n",
    "# calculate the distribution (%) of empty strings \"\" in specified columns\n",
    "empty_string_distribution = train_df[columns_to_check].map(lambda x: x == \"\").mean() * 100\n",
    "\n",
    "print(\"Distribution (%) of empty strings in specified columns:\")\n",
    "print(empty_string_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8029f54-7319-4163-860a-67fb32031d3a",
   "metadata": {},
   "source": [
    "### Prepare Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c394673-e09d-47fb-9685-ab5438fb5e7c",
   "metadata": {},
   "source": [
    "#### Configure Train Data: System Prompt, Instruction, Context and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fb78e3c-b671-4dbf-85f3-157a1bf63ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_version: version-2-Ryan\n"
     ]
    }
   ],
   "source": [
    "system_prompt = prompt_email_names[\"system_prompt\"]\n",
    "instruction = prompt_email_names[\"instruction\"]\n",
    "prompt_version = prompt_email_names[\"prompt_version\"]\n",
    "prompt_type = prompt_email_names[\"prompt_type\"]\n",
    "print(f'prompt_version: {prompt_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b1fd0e7-9344-4abb-b3e3-697387f18c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_context(email_address, display_name):\n",
    "    email_address = email_address.strip()\n",
    "    display_name = display_name.strip()\n",
    "    \n",
    "    context_input_str = f\"\"\"Input:\"\"\"\n",
    "    context_data = f\"\"\"{{\"Email Address\": \"{email_address}\", \"Display Name\": \"{display_name}\"}}\"\"\"\n",
    "    context = context_input_str.strip() + context_data.strip()\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "contexts = train_df.apply(lambda x: get_context(x['Email Address'], x['Display Name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08f7f02c-a295-44ff-bb92-9fca6e822cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input:{\"Email Address\": \"a_anderson@allianca.com\", \"Display Name\": \"Alex Anderson\"}'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8754a70-1adc-47c0-8148-6b54f11558d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(first_name, mid_name, last_name, name_prefix, name_suffix):\n",
    "    first_name = first_name.strip()\n",
    "    mid_name = mid_name.strip()\n",
    "    last_name = last_name.strip()\n",
    "    name_prefix = name_prefix.strip()\n",
    "    name_suffix = name_suffix.strip()\n",
    "    \n",
    "    output = f'''\\nFirst Name: {first_name}\\nMiddle Name: {mid_name}\\nLast Name: {last_name}\\nName Prefix: {name_prefix}\\nName Suffix: {name_suffix}'''\n",
    "    return output\n",
    "\n",
    "responses = train_df.apply(lambda x: get_response(x['First Name'], x['Middle Name'], x['Last Name'], x['Name Prefix'], x['Name Suffix']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "517efef8-ba30-472f-8a24-49eeec6130be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFirst Name: Alex\\nMiddle Name: \\nLast Name: Anderson\\nName Prefix: \\nName Suffix: '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0eb40023-a4ba-4c48-892f-3bf33d617b01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a highly skilled assistant specializin...</td>\n",
       "      <td>Please extract the email name components from ...</td>\n",
       "      <td>Input:{\"Email Address\": \"a_anderson@allianca.c...</td>\n",
       "      <td>\\nFirst Name: Alex\\nMiddle Name: \\nLast Name: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a highly skilled assistant specializin...</td>\n",
       "      <td>Please extract the email name components from ...</td>\n",
       "      <td>Input:{\"Email Address\": \"a_anderson@alliant.co...</td>\n",
       "      <td>\\nFirst Name: Alex\\nMiddle Name: \\nLast Name: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a highly skilled assistant specializin...</td>\n",
       "      <td>Please extract the email name components from ...</td>\n",
       "      <td>Input:{\"Email Address\": \"a_bell@carpenterfarra...</td>\n",
       "      <td>\\nFirst Name: Anna\\nMiddle Name: L.\\nLast Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a highly skilled assistant specializin...</td>\n",
       "      <td>Please extract the email name components from ...</td>\n",
       "      <td>Input:{\"Email Address\": \"a_bodnar@orquest.com\"...</td>\n",
       "      <td>\\nFirst Name: Akshay\\nMiddle Name: \\nLast Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a highly skilled assistant specializin...</td>\n",
       "      <td>Please extract the email name components from ...</td>\n",
       "      <td>Input:{\"Email Address\": \"a_brown@onesteamboatp...</td>\n",
       "      <td>\\nFirst Name: Adam\\nMiddle Name: \\nLast Name: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       system_prompt  \\\n",
       "0  You are a highly skilled assistant specializin...   \n",
       "1  You are a highly skilled assistant specializin...   \n",
       "2  You are a highly skilled assistant specializin...   \n",
       "3  You are a highly skilled assistant specializin...   \n",
       "4  You are a highly skilled assistant specializin...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Please extract the email name components from ...   \n",
       "1  Please extract the email name components from ...   \n",
       "2  Please extract the email name components from ...   \n",
       "3  Please extract the email name components from ...   \n",
       "4  Please extract the email name components from ...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Input:{\"Email Address\": \"a_anderson@allianca.c...   \n",
       "1  Input:{\"Email Address\": \"a_anderson@alliant.co...   \n",
       "2  Input:{\"Email Address\": \"a_bell@carpenterfarra...   \n",
       "3  Input:{\"Email Address\": \"a_bodnar@orquest.com\"...   \n",
       "4  Input:{\"Email Address\": \"a_brown@onesteamboatp...   \n",
       "\n",
       "                                            response  \n",
       "0  \\nFirst Name: Alex\\nMiddle Name: \\nLast Name: ...  \n",
       "1  \\nFirst Name: Alex\\nMiddle Name: \\nLast Name: ...  \n",
       "2  \\nFirst Name: Anna\\nMiddle Name: L.\\nLast Name...  \n",
       "3  \\nFirst Name: Akshay\\nMiddle Name: \\nLast Name...  \n",
       "4  \\nFirst Name: Adam\\nMiddle Name: \\nLast Name: ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_train_df = pd.DataFrame({'system_prompt':system_prompt,\n",
    "                         'instruction':instruction,\n",
    "                          'context': contexts,\n",
    "                         'response':responses\n",
    "                        })\n",
    "prompt_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a69502c4-bf06-4ce1-b798-15260de9af1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump train JSONL locally\n",
    "output_data_dir = '../data'\n",
    "train_filename = f'{output_data_dir}/mistral-7b-fine-tuning-dataset-{prompt_version}-{file_name}.jsonl'\n",
    "with open(train_filename, \"w\") as f:\n",
    "    f.write(prompt_train_df.to_json(orient='records', lines=True, force_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc274716-269a-4b12-a78d-1949fc14f24b",
   "metadata": {},
   "source": [
    "#### Upload Train JSONL and Template to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24e45ef3-1156-46b5-8c54-00efdd5386b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "    \"prompt\": \"{system_prompt}\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\",\n",
    "}\n",
    "\n",
    "with open(f'{output_data_dir}/template.json', 'w') as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b8113c3-055b-4c44-a2c0-b373e18cc1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Training data uploaded to: s3://sagemaker-sigparser-caylent-mlops/data/email-names/input/training/Mistral-7B/2024-04-08_18-48-25\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model = 'Mistral-7B'\n",
    "s3_train_input_prefix = 'data/email-names/input/training'\n",
    "\n",
    "object_name = f'{s3_train_input_prefix}/{model}/{timestamp}'\n",
    "\n",
    "# create the file name as per the task: name-parse, email-signature\n",
    "file_name = train_filename\n",
    "train_data_location = f's3://{sess.default_bucket()}/{object_name}'\n",
    "\n",
    "S3Uploader.upload(file_name, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "                  \n",
    "print(f\"Training data uploaded to: s3://{sess.default_bucket()}/{object_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a19bd-1dbc-465d-9ba0-cabe5349e29a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794bab91-f052-4dd2-a752-65991423db8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-llm-mistral-7b\", \"2.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8d632-5914-4d7b-a452-a3990d116c85",
   "metadata": {},
   "source": [
    "#### Training Hyperparameters\n",
    "\n",
    "- We can use the default hyperparameters but overwrite if needed.\n",
    "- Note: for now, we will not use `LoRA` for fine-tuning but this can be changed later on as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9ad1003-82c7-44a6-a3ca-d2ba4faa912a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mistral Training Hyperparameters: {'peft_type': 'None', 'instruction_tuned': 'True', 'chat_dataset': 'False', 'epoch': '1', 'learning_rate': '6e-06', 'lora_r': '64', 'lora_alpha': '16', 'lora_dropout': '0', 'bits': '16', 'double_quant': 'True', 'quant_type': 'nf4', 'per_device_train_batch_size': '2', 'per_device_eval_batch_size': '8', 'add_input_output_demarcation_key': 'True', 'warmup_ratio': '0.1', 'train_from_scratch': 'False', 'fp16': 'False', 'bf16': 'True', 'evaluation_strategy': 'steps', 'eval_steps': '20', 'gradient_accumulation_steps': '8', 'logging_steps': '8', 'weight_decay': '0.2', 'load_best_model_at_end': 'True', 'max_train_samples': '-1', 'max_val_samples': '-1', 'seed': '10', 'max_input_length': '-1', 'validation_split_ratio': '0.2', 'train_data_split_seed': '0', 'preprocessing_num_workers': 'None', 'max_steps': '-1', 'gradient_checkpointing': 'True', 'early_stopping_patience': '3', 'early_stopping_threshold': '0.0', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'max_grad_norm': '1.0', 'label_smoothing_factor': '0', 'logging_first_step': 'False', 'logging_nan_inf_filter': 'True', 'save_strategy': 'steps', 'save_steps': '500', 'save_total_limit': '1', 'dataloader_drop_last': 'False', 'dataloader_num_workers': '0', 'eval_accumulation_steps': 'None', 'auto_find_batch_size': 'False', 'lr_scheduler_type': 'constant_with_warmup', 'warmup_steps': '0'}\n"
     ]
    }
   ],
   "source": [
    "mistral_hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "print(f'Default Mistral Training Hyperparameters: {mistral_hyperparameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a63e6c-69ca-493f-8d52-4687f89c58e7",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc6d37-280b-40b7-85a4-fc9d957bc588",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: hf-llm-mistral-7b-2024-04-08-18-50-53-623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-08 18:50:53 Starting - Starting the training job...\n",
      "2024-04-08 18:51:07 Pending - Training job waiting for capacity...\n",
      "2024-04-08 18:51:34 Pending - Preparing the instances for training......\n",
      "2024-04-08 18:52:30 Downloading - Downloading input data.......................................\n",
      "2024-04-08 18:59:14 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:16,381 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:16,435 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:16,444 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:16,446 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:18,823 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.26.1-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.10.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.15-py3-none-any.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/flash_attn/flash_attn-2.5.5-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ninja/ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/packaging/packaging-23.2-py3-none-any.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.8.2-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py_cpuinfo/py_cpuinfo-9.0.0-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/rich/rich-13.7.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.6.5-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.38.1-py3-none-any.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.7.10-py3-none-any.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.2-py3-none-any.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.2-py2.py3-none-any.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.10-py2.py3-none-any.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.5->-r requirements.txt (line 5)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich==13.7.0->-r requirements.txt (line 10)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich==13.7.0->-r requirements.txt (line 10)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 14)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 14)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 14)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.7.10->-r requirements.txt (line 15)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.2->-r requirements.txt (line 16)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.26.1->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich==13.7.0->-r requirements.txt (line 10)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.16.0)\u001b[0m\n",
      "\u001b[34mninja is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mpy-cpuinfo is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.10.3-py3-none-any.whl size=907844 sha256=626cba16e272c8de1ca49014e4d2fd5c818781d9f781eeec1dd49f58d8d29583\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b4/a0/a9/4723ccba9b5790d90f40617f369a69c6dff729fa4b0aa6e131\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: shtab, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, packaging, docstring-parser, rich, bitsandbytes, tyro, tokenizers, flash-attn, deepspeed, accelerate, transformers, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: packaging\u001b[0m\n",
      "\u001b[34mFound existing installation: packaging 23.1\u001b[0m\n",
      "\u001b[34mUninstalling packaging-23.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled packaging-23.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: rich\u001b[0m\n",
      "\u001b[34mFound existing installation: rich 13.4.2\u001b[0m\n",
      "\u001b[34mUninstalling rich-13.4.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled rich-13.4.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.26.1 bitsandbytes-0.42.0 deepspeed-0.10.3 docstring-parser-0.15 flash-attn-2.5.5 packaging-23.2 peft-0.8.2 rich-13.7.0 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.2 sagemaker-jumpstart-script-utilities-1.1.10 sagemaker-jumpstart-tabular-script-utilities-1.0.0 shtab-1.6.5 tokenizers-0.15.1 transformers-4.38.1 trl-0.7.10 tyro-0.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:46,695 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:46,695 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:46,799 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:46,893 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:46,986 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:46,996 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"adam_beta1\": \"0.9\",\n",
      "        \"adam_beta2\": \"0.999\",\n",
      "        \"adam_epsilon\": \"1e-08\",\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"auto_find_batch_size\": \"False\",\n",
      "        \"bf16\": \"True\",\n",
      "        \"bits\": \"16\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"dataloader_drop_last\": \"False\",\n",
      "        \"dataloader_num_workers\": \"0\",\n",
      "        \"double_quant\": \"True\",\n",
      "        \"early_stopping_patience\": \"3\",\n",
      "        \"early_stopping_threshold\": \"0.0\",\n",
      "        \"epoch\": \"2\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_steps\": \"20\",\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"fp16\": \"False\",\n",
      "        \"gradient_accumulation_steps\": \"8\",\n",
      "        \"gradient_checkpointing\": \"True\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"label_smoothing_factor\": \"0\",\n",
      "        \"learning_rate\": \"6e-06\",\n",
      "        \"load_best_model_at_end\": \"True\",\n",
      "        \"logging_first_step\": \"False\",\n",
      "        \"logging_nan_inf_filter\": \"True\",\n",
      "        \"logging_steps\": \"8\",\n",
      "        \"lora_alpha\": \"16\",\n",
      "        \"lora_dropout\": \"0\",\n",
      "        \"lora_r\": \"64\",\n",
      "        \"lr_scheduler_type\": \"constant_with_warmup\",\n",
      "        \"max_grad_norm\": \"1.0\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_steps\": \"-1\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"peft_type\": \"None\",\n",
      "        \"per_device_eval_batch_size\": \"8\",\n",
      "        \"per_device_train_batch_size\": \"2\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"quant_type\": \"nf4\",\n",
      "        \"save_steps\": \"500\",\n",
      "        \"save_strategy\": \"steps\",\n",
      "        \"save_total_limit\": \"1\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"train_from_scratch\": \"False\",\n",
      "        \"validation_split_ratio\": \"0.2\",\n",
      "        \"warmup_ratio\": \"0.1\",\n",
      "        \"warmup_steps\": \"0\",\n",
      "        \"weight_decay\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"hf-llm-mistral-7b-2024-04-08-18-50-53-623\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"add_input_output_demarcation_key\":\"True\",\"auto_find_batch_size\":\"False\",\"bf16\":\"True\",\"bits\":\"16\",\"chat_dataset\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"double_quant\":\"True\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"2\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"False\",\"gradient_accumulation_steps\":\"8\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"8\",\"lora_alpha\":\"16\",\"lora_dropout\":\"0\",\"lora_r\":\"64\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"1024\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"peft_type\":\"None\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"2\",\"preprocessing_num_workers\":\"None\",\"quant_type\":\"nf4\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"adam_beta1\":\"0.9\",\"adam_beta2\":\"0.999\",\"adam_epsilon\":\"1e-08\",\"add_input_output_demarcation_key\":\"True\",\"auto_find_batch_size\":\"False\",\"bf16\":\"True\",\"bits\":\"16\",\"chat_dataset\":\"False\",\"dataloader_drop_last\":\"False\",\"dataloader_num_workers\":\"0\",\"double_quant\":\"True\",\"early_stopping_patience\":\"3\",\"early_stopping_threshold\":\"0.0\",\"epoch\":\"2\",\"eval_accumulation_steps\":\"None\",\"eval_steps\":\"20\",\"evaluation_strategy\":\"steps\",\"fp16\":\"False\",\"gradient_accumulation_steps\":\"8\",\"gradient_checkpointing\":\"True\",\"instruction_tuned\":\"True\",\"label_smoothing_factor\":\"0\",\"learning_rate\":\"6e-06\",\"load_best_model_at_end\":\"True\",\"logging_first_step\":\"False\",\"logging_nan_inf_filter\":\"True\",\"logging_steps\":\"8\",\"lora_alpha\":\"16\",\"lora_dropout\":\"0\",\"lora_r\":\"64\",\"lr_scheduler_type\":\"constant_with_warmup\",\"max_grad_norm\":\"1.0\",\"max_input_length\":\"1024\",\"max_steps\":\"-1\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"peft_type\":\"None\",\"per_device_eval_batch_size\":\"8\",\"per_device_train_batch_size\":\"2\",\"preprocessing_num_workers\":\"None\",\"quant_type\":\"nf4\",\"save_steps\":\"500\",\"save_strategy\":\"steps\",\"save_total_limit\":\"1\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"train_from_scratch\":\"False\",\"validation_split_ratio\":\"0.2\",\"warmup_ratio\":\"0.1\",\"warmup_steps\":\"0\",\"weight_decay\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"hf-llm-mistral-7b-2024-04-08-18-50-53-623\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--adam_beta1\",\"0.9\",\"--adam_beta2\",\"0.999\",\"--adam_epsilon\",\"1e-08\",\"--add_input_output_demarcation_key\",\"True\",\"--auto_find_batch_size\",\"False\",\"--bf16\",\"True\",\"--bits\",\"16\",\"--chat_dataset\",\"False\",\"--dataloader_drop_last\",\"False\",\"--dataloader_num_workers\",\"0\",\"--double_quant\",\"True\",\"--early_stopping_patience\",\"3\",\"--early_stopping_threshold\",\"0.0\",\"--epoch\",\"2\",\"--eval_accumulation_steps\",\"None\",\"--eval_steps\",\"20\",\"--evaluation_strategy\",\"steps\",\"--fp16\",\"False\",\"--gradient_accumulation_steps\",\"8\",\"--gradient_checkpointing\",\"True\",\"--instruction_tuned\",\"True\",\"--label_smoothing_factor\",\"0\",\"--learning_rate\",\"6e-06\",\"--load_best_model_at_end\",\"True\",\"--logging_first_step\",\"False\",\"--logging_nan_inf_filter\",\"True\",\"--logging_steps\",\"8\",\"--lora_alpha\",\"16\",\"--lora_dropout\",\"0\",\"--lora_r\",\"64\",\"--lr_scheduler_type\",\"constant_with_warmup\",\"--max_grad_norm\",\"1.0\",\"--max_input_length\",\"1024\",\"--max_steps\",\"-1\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--peft_type\",\"None\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"2\",\"--preprocessing_num_workers\",\"None\",\"--quant_type\",\"nf4\",\"--save_steps\",\"500\",\"--save_strategy\",\"steps\",\"--save_total_limit\",\"1\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--train_from_scratch\",\"False\",\"--validation_split_ratio\",\"0.2\",\"--warmup_ratio\",\"0.1\",\"--warmup_steps\",\"0\",\"--weight_decay\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_BETA2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_ADAM_EPSILON=1e-08\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_FIND_BATCH_SIZE=False\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=True\u001b[0m\n",
      "\u001b[34mSM_HP_BITS=16\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_DROP_LAST=False\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=0\u001b[0m\n",
      "\u001b[34mSM_HP_DOUBLE_QUANT=True\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_THRESHOLD=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=2\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=20\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=False\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=8\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=True\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING_FACTOR=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=6e-06\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FIRST_STEP=False\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_NAN_INF_FILTER=True\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=8\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=16\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=64\u001b[0m\n",
      "\u001b[34mSM_HP_LR_SCHEDULER_TYPE=constant_with_warmup\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_GRAD_NORM=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PEFT_TYPE=None\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_QUANT_TYPE=nf4\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FROM_SCRATCH=False\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_RATIO=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --adam_beta1 0.9 --adam_beta2 0.999 --adam_epsilon 1e-08 --add_input_output_demarcation_key True --auto_find_batch_size False --bf16 True --bits 16 --chat_dataset False --dataloader_drop_last False --dataloader_num_workers 0 --double_quant True --early_stopping_patience 3 --early_stopping_threshold 0.0 --epoch 2 --eval_accumulation_steps None --eval_steps 20 --evaluation_strategy steps --fp16 False --gradient_accumulation_steps 8 --gradient_checkpointing True --instruction_tuned True --label_smoothing_factor 0 --learning_rate 6e-06 --load_best_model_at_end True --logging_first_step False --logging_nan_inf_filter True --logging_steps 8 --lora_alpha 16 --lora_dropout 0 --lora_r 64 --lr_scheduler_type constant_with_warmup --max_grad_norm 1.0 --max_input_length 1024 --max_steps -1 --max_train_samples -1 --max_val_samples -1 --peft_type None --per_device_eval_batch_size 8 --per_device_train_batch_size 2 --preprocessing_num_workers None --quant_type nf4 --save_steps 500 --save_strategy steps --save_total_limit 1 --seed 10 --train_data_split_seed 0 --train_from_scratch False --validation_split_ratio 0.2 --warmup_ratio 0.1 --warmup_steps 0 --weight_decay 0.2\u001b[0m\n",
      "\u001b[34m2024-04-08 18:59:47,026 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mFound existing installation: diffusers 0.16.1\u001b[0m\n",
      "\u001b[34mUninstalling diffusers-0.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled diffusers-0.16.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2024-04-08 18:59:50,542] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(model_dir='/opt/ml/model', train='/opt/ml/input/data/training', train_alt=None, validation=None, hosts=['algo-1'], num_gpus=4, current_host='algo-1', pretrained_model='/opt/ml/input/data/model', peft_type='None', lora_r=64, lora_alpha=16, lora_dropout=0.0, bits=16, double_quant=True, quant_type='nf4', deepspeed=True, instruction_tuned='True', chat_dataset='False', train_from_scratch='False', fp16='False', bf16='True', evaluation_strategy='steps', eval_steps=20, epoch=2, gradient_accumulation_steps=8, per_device_train_batch_size=2, per_device_eval_batch_size=8, logging_steps=8, warmup_ratio=0.1, learning_rate=6e-06, weight_decay=0.2, load_best_model_at_end='True', max_train_samples=-1, max_val_samples=-1, seed=10, max_input_length=1024, validation_split_ratio=0.2, train_data_split_seed=0, preprocessing_num_workers=None, max_steps=-1, gradient_checkpointing='True', early_stopping_patience=3, early_stopping_threshold=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0.0, logging_strategy='steps', logging_first_step='False', logging_nan_inf_filter='True', save_strategy='steps', save_steps=500, save_total_limit=1, dataloader_drop_last='False', dataloader_num_workers=0, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='constant_with_warmup', warmup_steps=0, add_input_output_demarcation_key='True').\u001b[0m\n",
      "\u001b[34mINFO:root:Ignoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mINFO:root:Uncompressing the input model tarball.\u001b[0m\n",
      "\u001b[34mINFO:root:Parameter 'instruction_tuned' is 'True'. Starting instruction fine-tuning.\u001b[0m\n",
      "\u001b[34mINFO:root:Running command ['deepspeed', '--num_gpus=4', '/opt/conda/lib/python3.10/site-packages/sagemaker_jumpstart_huggingface_script_utilities/fine_tuning/run_clm.py', '--deepspeed', 'ds_config.json', '--model_name_or_path', '/tmp', '--train_file', '/opt/ml/input/data/training', '--do_train', '--output_dir', '/opt/ml/model', '--num_train_epochs', '2', '--gradient_accumulation_steps', '8', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '8', '--logging_steps', '8', '--warmup_ratio', '0.1', '--learning_rate', '6e-06', '--weight_decay', '0.2', '--seed', '10', '--max_input_length', '1024', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--max_steps', '-1', '--early_stopping_patience', '3', '--early_stopping_threshold', '0.0', '--adam_beta1', '0.9', '--adam_beta2', '0.999', '--max_grad_norm', '1.0', '--label_smoothing_factor', '0.0', '--logging_strategy', 'steps', '--save_strategy', 'steps', '--save_steps', '500', '--dataloader_num_workers', '0', '--lr_scheduler_type', 'constant_with_warmup', '--warmup_steps', '0', '--evaluation_strategy', 'steps', '--eval_steps', '20', '--lora_r', '64', '--lora_alpha', '16', '--lora_dropout', '0.0', '--bits', '16', '--quant_type', 'nf4', '--add_input_output_demarcation_key', 'True', '--load_best_model_at_end', '--bf16', '--instruction_tuned', '--gradient_checkpointing', '--save_total_limit', '1', '--double_quant']\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:44,346] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:47,071] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:47,072] [INFO] [runner.py:570:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /opt/conda/lib/python3.10/site-packages/sagemaker_jumpstart_huggingface_script_utilities/fine_tuning/run_clm.py --deepspeed ds_config.json --model_name_or_path /tmp --train_file /opt/ml/input/data/training --do_train --output_dir /opt/ml/model --num_train_epochs 2 --gradient_accumulation_steps 8 --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --logging_steps 8 --warmup_ratio 0.1 --learning_rate 6e-06 --weight_decay 0.2 --seed 10 --max_input_length 1024 --validation_split_ratio 0.2 --train_data_split_seed 0 --max_steps -1 --early_stopping_patience 3 --early_stopping_threshold 0.0 --adam_beta1 0.9 --adam_beta2 0.999 --max_grad_norm 1.0 --label_smoothing_factor 0.0 --logging_strategy steps --save_strategy steps --save_steps 500 --dataloader_num_workers 0 --lr_scheduler_type constant_with_warmup --warmup_steps 0 --evaluation_strategy steps --eval_steps 20 --lora_r 64 --lora_alpha 16 --lora_dropout 0.0 --bits 16 --quant_type nf4 --add_input_output_demarcation_key True --load_best_model_at_end --bf16 --instruction_tuned --gradient_checkpointing --save_total_limit 1 --double_quant\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:48,653] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:163:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:51,346] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:57,115] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:57,117] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:57,120] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:01:57,125] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:00,328] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:00,339] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:00,347] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:00,347] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:00,369] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:01 - WARNING - jumpstart -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:01 - WARNING - jumpstart -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:01 - WARNING - jumpstart -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:01 - WARNING - jumpstart -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:01 - INFO - jumpstart -   Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=True,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=ds_config.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=20,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=steps,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=8,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=True,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=6e-06,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Apr08_19-02-00_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=8,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=constant_with_warmup,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=2.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=1,\u001b[0m\n",
      "\u001b[34mseed=10,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.1,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.2,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-04-08 19:02:01,579 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:726] 2024-04-08 19:02:01,652 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:726] 2024-04-08 19:02:01,652 >> loading configuration file /tmp/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:791] 2024-04-08 19:02:01,653 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:791] 2024-04-08 19:02:01,653 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/tmp\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:01 - INFO - jumpstart -   Overwrite use_cache to be False in the model config.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3254] 2024-04-08 19:02:01,688 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3254] 2024-04-08 19:02:01,688 >> loading weights file /tmp/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1400] 2024-04-08 19:02:01,688 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1400] 2024-04-08 19:02:01,688 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3363] 2024-04-08 19:02:01,688 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3363] 2024-04-08 19:02:01,688 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:329] 2024-04-08 19:02:01,690 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:329] 2024-04-08 19:02:01,690 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-04-08 19:02:01,694 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-04-08 19:02:01,694 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mNCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:08,078] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 7.24B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:10<00:10, 10.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:10<00:10, 10.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:10<00:10, 10.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:11<00:11, 11.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:16<00:00,  7.63s/it]#015Loading checkpoint shards: 100%|| 2/2 [00:16<00:00,  7.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:16<00:00,  8.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:16<00:00,  8.10s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|| 2/2 [00:16<00:00,  7.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:16<00:00,  8.10s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 21150 examples [00:00, 250793.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:16<00:00,  7.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:16<00:00,  8.21s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3992] 2024-04-08 19:02:24,538 >> All model checkpoint weights were used when initializing MistralForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3992] 2024-04-08 19:02:24,538 >> All model checkpoint weights were used when initializing MistralForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4000] 2024-04-08 19:02:24,539 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4000] 2024-04-08 19:02:24,539 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /tmp.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:798] 2024-04-08 19:02:24,542 >> loading configuration file /tmp/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:798] 2024-04-08 19:02:24,542 >> loading configuration file /tmp/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-04-08 19:02:24,542 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-04-08 19:02:24,542 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:24 - INFO - jumpstart -   Training data is identified. The corresponded column names are ['system_prompt', 'instruction', 'context', 'response'].\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:24 - INFO - jumpstart -   The max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  47%|     | 10000/21150 [00:00<00:00, 83231.38 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  43%|     | 9000/21150 [00:00<00:00, 80859.42 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  43%|     | 9000/21150 [00:00<00:00, 79726.32 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  43%|     | 9000/21150 [00:00<00:00, 81219.55 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  95%|| 20000/21150 [00:00<00:00, 84643.98 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  90%| | 19000/21150 [00:00<00:00, 83636.93 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  90%| | 19000/21150 [00:00<00:00, 81962.30 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template: 100%|| 21150/21150 [00:00<00:00, 83702.37 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template: 100%|| 21150/21150 [00:00<00:00, 82773.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template: 100%|| 21150/21150 [00:00<00:00, 81141.54 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:  85%| | 18000/21150 [00:00<00:00, 81384.36 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template: 100%|| 21150/21150 [00:00<00:00, 78819.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/21150 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|         | 1000/21150 [00:00<00:06, 3173.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|         | 1000/21150 [00:00<00:06, 3273.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|         | 1000/21150 [00:00<00:06, 3169.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|         | 1000/21150 [00:00<00:05, 3430.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   9%|         | 2000/21150 [00:00<00:05, 3694.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   9%|         | 2000/21150 [00:00<00:05, 3547.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   9%|         | 2000/21150 [00:00<00:05, 3691.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   9%|         | 2000/21150 [00:00<00:05, 3823.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  14%|        | 3000/21150 [00:00<00:04, 3942.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  14%|        | 3000/21150 [00:00<00:04, 3844.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  14%|        | 3000/21150 [00:00<00:04, 3735.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  14%|        | 3000/21150 [00:00<00:04, 3993.09 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  19%|        | 4000/21150 [00:01<00:04, 4112.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  19%|        | 4000/21150 [00:01<00:04, 3979.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  19%|        | 4000/21150 [00:01<00:04, 3885.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  19%|        | 4000/21150 [00:01<00:04, 4093.10 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  24%|       | 5000/21150 [00:01<00:03, 4244.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  24%|       | 5000/21150 [00:01<00:04, 4000.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  24%|       | 5000/21150 [00:01<00:04, 3934.28 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  24%|       | 5000/21150 [00:01<00:03, 4108.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  28%|       | 6000/21150 [00:01<00:03, 4308.13 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  28%|       | 6000/21150 [00:01<00:03, 4060.35 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  28%|       | 6000/21150 [00:01<00:03, 4020.18 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  28%|       | 6000/21150 [00:01<00:03, 4133.98 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|      | 7000/21150 [00:01<00:03, 4360.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|      | 7000/21150 [00:01<00:03, 4092.25 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|      | 7000/21150 [00:01<00:03, 4070.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|      | 7000/21150 [00:01<00:03, 4202.61 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|      | 8000/21150 [00:01<00:03, 4376.07 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|      | 8000/21150 [00:02<00:03, 4087.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|      | 8000/21150 [00:02<00:03, 4083.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|      | 8000/21150 [00:01<00:03, 4214.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  43%|     | 9000/21150 [00:02<00:02, 4424.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  43%|     | 9000/21150 [00:02<00:02, 4120.92 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  43%|     | 9000/21150 [00:02<00:02, 4111.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  43%|     | 9000/21150 [00:02<00:02, 4227.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  47%|     | 10000/21150 [00:02<00:02, 4450.47 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  47%|     | 10000/21150 [00:02<00:02, 4143.02 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  47%|     | 10000/21150 [00:02<00:02, 4132.15 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  47%|     | 10000/21150 [00:02<00:02, 4222.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|    | 11000/21150 [00:02<00:02, 4437.74 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|    | 11000/21150 [00:02<00:02, 4146.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|    | 11000/21150 [00:02<00:02, 4145.34 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|    | 11000/21150 [00:02<00:02, 4208.19 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|    | 12000/21150 [00:02<00:02, 4460.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|    | 12000/21150 [00:02<00:02, 4151.91 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|    | 12000/21150 [00:02<00:02, 4168.33 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|    | 12000/21150 [00:02<00:02, 4226.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  61%|   | 13000/21150 [00:03<00:01, 4466.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  61%|   | 13000/21150 [00:03<00:01, 4190.44 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  61%|   | 13000/21150 [00:03<00:01, 4168.69 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|   | 14000/21150 [00:03<00:01, 4436.96 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  61%|   | 13000/21150 [00:03<00:01, 4204.94 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|   | 14000/21150 [00:03<00:01, 4186.45 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|   | 14000/21150 [00:03<00:01, 4148.58 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|   | 15000/21150 [00:03<00:01, 4446.29 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|   | 14000/21150 [00:03<00:01, 4153.87 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|   | 15000/21150 [00:03<00:01, 4203.37 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|   | 15000/21150 [00:03<00:01, 4202.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|  | 16000/21150 [00:03<00:01, 4513.06 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|   | 15000/21150 [00:03<00:01, 4180.22 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|  | 17000/21150 [00:03<00:00, 4427.50 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|  | 16000/21150 [00:03<00:01, 4148.51 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|  | 16000/21150 [00:03<00:01, 4154.40 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|  | 16000/21150 [00:03<00:01, 4203.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%| | 18000/21150 [00:04<00:00, 4390.46 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|  | 17000/21150 [00:04<00:01, 4112.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|  | 17000/21150 [00:04<00:01, 4070.32 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|  | 17000/21150 [00:04<00:00, 4189.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  90%| | 19000/21150 [00:04<00:00, 4430.53 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%| | 18000/21150 [00:04<00:00, 4185.62 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%| | 18000/21150 [00:04<00:00, 4122.04 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%| | 18000/21150 [00:04<00:00, 4188.97 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  95%|| 20000/21150 [00:04<00:00, 4434.76 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  90%| | 19000/21150 [00:04<00:00, 4214.52 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  90%| | 19000/21150 [00:04<00:00, 4128.72 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  90%| | 19000/21150 [00:04<00:00, 4200.56 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|| 21000/21150 [00:04<00:00, 4470.78 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|| 21150/21150 [00:04<00:00, 4343.16 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  95%|| 20000/21150 [00:04<00:00, 4230.57 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  95%|| 20000/21150 [00:04<00:00, 4173.03 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  95%|| 20000/21150 [00:04<00:00, 4206.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|| 21000/21150 [00:05<00:00, 4266.93 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|| 21000/21150 [00:05<00:00, 4219.17 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|| 21000/21150 [00:05<00:00, 4287.42 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|| 21150/21150 [00:05<00:00, 4108.21 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|| 21150/21150 [00:05<00:00, 4070.20 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|| 21150/21150 [00:05<00:00, 4166.06 examples/s]\u001b[0m\n",
      "\u001b[34m04/08/2024 19:02:29 - INFO - jumpstart -   Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1875] 2024-04-08 19:02:29,947 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32004. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1875] 2024-04-08 19:02:29,947 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32004. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:601] 2024-04-08 19:02:30,400 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:601] 2024-04-08 19:02:30,400 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:30,525] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:02:30,545] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/4] /opt/conda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.049318075180054 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.114672899246216 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.11566662788391 seconds\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.116130113601685 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000006, betas=(0.900000, 0.999000), weight_decay=0.200000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,641] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,656] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,656] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,656] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,656] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,772] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,773] [INFO] [utils.py:804:see_memory_usage] MA 0.99 GB         Max_MA 1.48 GB         CA 1.07 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,773] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 49.14 GB, percent = 13.1%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,775] [INFO] [stage3.py:126:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,775] [INFO] [stage3.py:127:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,890] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,891] [INFO] [utils.py:804:see_memory_usage] MA 0.99 GB         Max_MA 0.99 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:02,891] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 49.14 GB, percent = 13.1%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:03,234] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:03,234] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.99 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:03,235] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 49.63 GB, percent = 13.3%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:03,353] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:03,354] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:03,354] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 49.62 GB, percent = 13.3%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,076] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,077] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,077] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 79.16 GB, percent = 21.2%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,198] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,198] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,199] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 75.64 GB, percent = 20.2%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,909] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,909] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:07,909] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 81.88 GB, percent = 21.9%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:08,264] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:08,265] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:08,265] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 93.0 GB, percent = 24.9%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:12,763] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:12,763] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:12,763] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 162.97 GB, percent = 43.6%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:12,764] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,358] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,359] [INFO] [utils.py:804:see_memory_usage] MA 0.53 GB         Max_MA 1.02 GB         CA 1.31 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,359] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 176.58 GB, percent = 47.2%\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,359] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,359] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,360] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9373f69f90>\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[6e-06], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,360] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   bfloat16_enabled ............. True\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9633f13f70>\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,361] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 8\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 1\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   loss_scale ................... 1.0\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   optimizer_params ............. {'lr': 6e-06, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.2}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 6e-06, 'warmup_num_steps': 53}\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  2\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,362] [INFO] [config.py:971:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,363] [INFO] [config.py:971:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,363] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,363] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,363] [INFO] [config.py:971:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,363] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,363] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-04-08 19:03:16,363] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 6e-06, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.2\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 6e-06, \n",
      "            \"warmup_num_steps\": 53\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1812] 2024-04-08 19:03:16,363 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1813] 2024-04-08 19:03:16,363 >>   Num examples = 16,920\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1812] 2024-04-08 19:03:16,363 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1813] 2024-04-08 19:03:16,363 >>   Num examples = 16,920\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1814] 2024-04-08 19:03:16,363 >>   Num Epochs = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1814] 2024-04-08 19:03:16,363 >>   Num Epochs = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1815] 2024-04-08 19:03:16,363 >>   Instantaneous batch size per device = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1818] 2024-04-08 19:03:16,363 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1819] 2024-04-08 19:03:16,363 >>   Gradient Accumulation steps = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1820] 2024-04-08 19:03:16,363 >>   Total optimization steps = 528\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1815] 2024-04-08 19:03:16,363 >>   Instantaneous batch size per device = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1818] 2024-04-08 19:03:16,363 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1819] 2024-04-08 19:03:16,363 >>   Gradient Accumulation steps = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1820] 2024-04-08 19:03:16,363 >>   Total optimization steps = 528\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1821] 2024-04-08 19:03:16,364 >>   Number of trainable parameters = 7,241,764,864\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1821] 2024-04-08 19:03:16,364 >>   Number of trainable parameters = 7,241,764,864\u001b[0m\n",
      "\u001b[34m0%|          | 0/528 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/528 [01:36<14:10:32, 96.84s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/528 [03:11<13:56:44, 95.45s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/528 [04:45<13:50:22, 94.90s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/528 [06:19<13:46:14, 94.61s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/528 [07:54<13:43:50, 94.51s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/528 [09:25<13:32:23, 93.38s/it]\u001b[0m\n",
      "\u001b[34m1%|         | 7/528 [10:57<13:28:35, 93.12s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 8/528 [12:29<13:23:53, 92.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.666, 'grad_norm': 8.610068291582696, 'learning_rate': 3.1425017408648087e-06, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m2%|         | 8/528 [12:29<13:23:53, 92.76s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 9/528 [14:02<13:21:30, 92.66s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 10/528 [15:34<13:18:01, 92.44s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 11/528 [17:07<13:17:53, 92.60s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 12/528 [18:40<13:18:42, 92.87s/it]\u001b[0m\n",
      "\u001b[34m2%|         | 13/528 [20:14<13:19:54, 93.19s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 14/528 [21:48<13:19:19, 93.31s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 15/528 [23:22<13:19:16, 93.48s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 16/528 [24:54<13:14:16, 93.08s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0825, 'grad_norm': 0.9762973730679647, 'learning_rate': 4.190002321153078e-06, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m3%|         | 16/528 [24:54<13:14:16, 93.08s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 17/528 [26:26<13:09:52, 92.75s/it]\u001b[0m\n",
      "\u001b[34m3%|         | 18/528 [27:59<13:09:17, 92.86s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 19/528 [29:32<13:08:39, 92.97s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 20/528 [31:03<13:03:17, 92.52s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 19:34:20,332 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 19:34:20,332 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 19:34:20,332 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 19:34:20,332 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 19:34:20,332 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 19:34:20,332 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:30,  1.61s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:06<05:13,  2.41s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:07,  2.85s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:13<06:37,  3.11s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:55,  3.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:04,  3.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:24<07:06,  3.41s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:09,  3.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:31<07:11,  3.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:10,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:38<07:06,  3.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:04,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:03,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:49<07:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:56,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:56<06:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:51,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:03<06:47,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:14<06:36,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:33,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:21<06:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:24,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:28<06:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:17,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:13,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:39<06:10,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:07,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:46<06:02,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<05:58,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:53<05:55,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:50,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:04<05:45,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:41,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:11<05:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:18<05:32,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:29<05:21,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:17,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:36<05:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:43<05:05,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:02,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:00,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:54<04:57,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:54,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:01<04:50,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:08<04:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:40,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:36,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:19<04:32,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:28,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:26<04:24,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:21,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:13,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:10,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:44<04:07,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:04,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:51<04:00,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:56,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:53,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:02<03:49,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:45,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:09<03:41,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:39,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:35,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:32,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:27,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:27<03:23,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:20,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:34<03:17,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:13,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:09,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:05,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:02,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:52<02:58,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:54,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [04:59<02:50,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:44,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:40,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:37,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:17<02:34,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:30,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:24<02:26,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:23,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:19,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:35<02:16,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:42<02:08,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:05,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:49<02:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:54,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:00<01:50,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:07<01:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:33,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:25<01:25,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:32<01:18,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:15,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:43<01:08,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:50<01:00,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:58<00:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:08<00:42,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:15<00:35,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:32,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:23<00:28,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:33<00:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:40<00:10,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:47<00:03,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.07216442376375198, 'eval_runtime': 475.7213, 'eval_samples_per_second': 8.892, 'eval_steps_per_second': 0.28, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m4%|         | 20/528 [38:59<13:03:17, 92.52s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 21/528 [40:31<33:05:39, 234.99s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 22/528 [42:05<27:05:44, 192.78s/it]\u001b[0m\n",
      "\u001b[34m4%|         | 23/528 [43:38<22:51:37, 162.97s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 24/528 [45:12<19:53:53, 142.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.073, 'grad_norm': 23.47139817496175, 'learning_rate': 4.802750880105367e-06, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m5%|         | 24/528 [45:12<19:53:53, 142.13s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 25/528 [46:45<17:47:21, 127.32s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 26/528 [48:17<16:17:40, 116.85s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 27/528 [49:49<15:13:41, 109.42s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 28/528 [51:21<14:28:16, 104.19s/it]\u001b[0m\n",
      "\u001b[34m5%|         | 29/528 [52:51<13:51:18, 99.96s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 30/528 [54:26<13:35:30, 98.25s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 31/528 [55:57<13:16:55, 96.21s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 32/528 [57:27<13:00:16, 94.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0927, 'grad_norm': 4.684612452906146, 'learning_rate': 5.237502901441348e-06, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m6%|         | 32/528 [57:27<13:00:16, 94.39s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 33/528 [58:59<12:52:44, 93.66s/it]\u001b[0m\n",
      "\u001b[34m6%|         | 34/528 [1:00:31<12:45:44, 93.01s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 35/528 [1:02:03<12:41:56, 92.73s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 36/528 [1:03:34<12:37:11, 92.34s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 37/528 [1:05:07<12:37:57, 92.62s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 38/528 [1:06:38<12:30:35, 91.91s/it]\u001b[0m\n",
      "\u001b[34m7%|         | 39/528 [1:08:08<12:25:22, 91.46s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 40/528 [1:09:40<12:25:45, 91.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0704, 'grad_norm': 0.7933918618068742, 'learning_rate': 5.574722767646957e-06, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m8%|         | 40/528 [1:09:40<12:25:45, 91.69s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 20:12:57,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 20:12:57,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 20:12:57,176 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 20:12:57,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 20:12:57,176 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 20:12:57,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:55,  1.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:27,  2.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:17,  2.93s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:42,  3.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:58,  3.29s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:04,  3.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:24<07:07,  3.42s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:09,  3.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:11,  3.50s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:11,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:07,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:04,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:01,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:49<06:59,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:56,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:57<06:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:51,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:42,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:39,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:14<06:35,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:32,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:25,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:23,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:20,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:16,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:39<06:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:07,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:05,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<06:01,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:49,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:04<05:45,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:42,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:38,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:31,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:28,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:30<05:22,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:19,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:16,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:11,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:07,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:01,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:55<04:57,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:54,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:51,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:43,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:38,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:36,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:20<04:31,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:29,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:20,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:38<04:13,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:10,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:45<04:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:02,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<03:58,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:55,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:51,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:02<03:47,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:43,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:10<03:40,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:37,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:33,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:30,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:27,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:27<03:23,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:20,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:35<03:17,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:13,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:10,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:06,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:02,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:53<02:59,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:55,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [05:00<02:51,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:43,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:41,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:37,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:18<02:34,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:30,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:23,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:20,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:36<02:16,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:43<02:08,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:04,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:58,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:54,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:00<01:50,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:46,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:08<01:43,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:39,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:32,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:25<01:25,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:33<01:18,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:15,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:43<01:08,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:50<01:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:58<00:53,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:08<00:42,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:15<00:35,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:31,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:23<00:28,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:33<00:17,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:40<00:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:48<00:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.06571274995803833, 'eval_runtime': 475.209, 'eval_samples_per_second': 8.901, 'eval_steps_per_second': 0.28, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m8%|         | 40/528 [1:17:36<12:25:45, 91.69s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 41/528 [1:19:09<31:45:39, 234.78s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 42/528 [1:20:40<25:53:22, 191.77s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 43/528 [1:22:12<21:46:32, 161.63s/it]\u001b[0m\n",
      "\u001b[34m8%|         | 44/528 [1:23:43<18:54:27, 140.63s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 45/528 [1:25:15<16:54:33, 126.03s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 46/528 [1:26:47<15:29:01, 115.65s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 47/528 [1:28:20<14:33:28, 108.96s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 48/528 [1:29:53<13:52:41, 104.09s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0658, 'grad_norm': 0.6750481438245288, 'learning_rate': 5.850251460393637e-06, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m9%|         | 48/528 [1:29:53<13:52:41, 104.09s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 49/528 [1:31:26<13:25:27, 100.89s/it]\u001b[0m\n",
      "\u001b[34m9%|         | 50/528 [1:32:59<13:05:03, 98.54s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 51/528 [1:34:31<12:48:13, 96.63s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 52/528 [1:36:03<12:35:12, 95.20s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 53/528 [1:37:34<12:22:38, 93.81s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 54/528 [1:39:06<12:16:05, 93.18s/it]\u001b[0m\n",
      "\u001b[34m10%|         | 55/528 [1:40:37<12:10:21, 92.65s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 56/528 [1:42:09<12:06:56, 92.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0625, 'grad_norm': 0.6244379257973487, 'learning_rate': 6e-06, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m11%|         | 56/528 [1:42:09<12:06:56, 92.41s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 57/528 [1:43:40<12:03:30, 92.17s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 58/528 [1:45:13<12:03:39, 92.38s/it]\u001b[0m\n",
      "\u001b[34m11%|         | 59/528 [1:46:46<12:02:32, 92.44s/it]\u001b[0m\n",
      "\u001b[34m11%|        | 60/528 [1:48:19<12:02:16, 92.60s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 20:51:35,693 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 20:51:35,693 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 20:51:35,696 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 20:51:35,696 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 20:51:35,696 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 20:51:35,696 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:52,  1.78s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:28,  2.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:12,  2.89s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:40,  3.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:56,  3.28s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:05,  3.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:24<07:08,  3.43s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:10,  3.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:11,  3.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:10,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:08,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:06,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:02,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:49<06:57,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:54,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:56<06:51,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:49,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:46,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:41,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:14<06:38,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:32,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:30,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:27,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:22,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:20,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:16,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:39<06:11,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:08,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<06:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:56,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:04<05:45,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:42,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:38,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:36,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:33,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:29<05:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:18,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:13,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:00,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:55<04:57,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:55,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:51,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:47,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:34,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:20<04:31,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:29,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:25,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:21,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:16,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:12,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:45<04:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<04:00,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:56,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:52,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:03<03:49,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:46,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:10<03:42,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:39,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:35,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:21<03:30,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:26,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:28<03:23,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:20,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:35<03:17,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:13,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:10,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:46<03:05,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:53<02:58,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:55,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [05:00<02:51,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:48,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:44,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:11<02:40,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:38,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:18<02:34,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:30,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:27,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:29<02:23,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:19,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:36<02:16,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:43<02:09,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:05,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:01,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:54<01:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:54,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:01<01:50,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:46,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:08<01:43,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:39,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:19<01:33,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:26<01:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:33<01:18,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:37<01:15,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:44<01:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:51<01:00,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:58<00:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:09<00:42,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:16<00:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:31,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:23<00:28,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:34<00:17,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:41<00:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:48<00:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.0608939528465271, 'eval_runtime': 475.5894, 'eval_samples_per_second': 8.894, 'eval_steps_per_second': 0.28, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m11%|        | 60/528 [1:56:14<12:02:16, 92.60s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:52<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 61/528 [1:57:47<30:30:55, 235.24s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 62/528 [1:59:18<24:50:08, 191.86s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 63/528 [2:00:48<20:51:51, 161.53s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 64/528 [2:02:19<18:04:03, 140.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0609, 'grad_norm': 0.5349976381133877, 'learning_rate': 6e-06, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m12%|        | 64/528 [2:02:19<18:04:03, 140.18s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 65/528 [2:03:51<16:10:29, 125.77s/it]\u001b[0m\n",
      "\u001b[34m12%|        | 66/528 [2:05:23<14:51:08, 115.73s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 67/528 [2:06:55<13:54:52, 108.66s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 68/528 [2:08:26<13:12:52, 103.42s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 69/528 [2:09:58<12:45:03, 100.01s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 70/528 [2:11:31<12:26:17, 97.77s/it]\u001b[0m\n",
      "\u001b[34m13%|        | 71/528 [2:13:04<12:12:59, 96.24s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 72/528 [2:14:35<11:59:07, 94.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0607, 'grad_norm': 0.5589879652290556, 'learning_rate': 6e-06, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m14%|        | 72/528 [2:14:35<11:59:07, 94.62s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 73/528 [2:16:06<11:51:07, 93.77s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 74/528 [2:17:37<11:43:18, 92.95s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 75/528 [2:19:09<11:39:26, 92.64s/it]\u001b[0m\n",
      "\u001b[34m14%|        | 76/528 [2:20:41<11:36:05, 92.40s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 77/528 [2:22:13<11:32:22, 92.11s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 78/528 [2:23:44<11:28:33, 91.81s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 79/528 [2:25:17<11:31:27, 92.40s/it]\u001b[0m\n",
      "\u001b[34m15%|        | 80/528 [2:26:51<11:32:38, 92.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0594, 'grad_norm': 0.5873992488586123, 'learning_rate': 6e-06, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m15%|        | 80/528 [2:26:51<11:32:38, 92.76s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 21:30:07,946 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 21:30:07,946 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 21:30:07,946 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 21:30:07,946 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 21:30:07,946 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 21:30:07,946 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:55,  1.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:30,  2.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:17,  2.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:44,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:58,  3.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:05,  3.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:25<07:10,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:11,  3.48s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:13,  3.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:11,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:07,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:03,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:01,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:49<06:58,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:56,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:57<06:52,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:49,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:45,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:42,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:39,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:14<06:36,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:34,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:25,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:19,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:15,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:40<06:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:09,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:04,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<06:01,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:59,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:48,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:05<05:45,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:42,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:36,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:33,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:23<05:30,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:30<05:20,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:16,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:11,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:01,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:55<04:57,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:52,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:42,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:13<04:38,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:35,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:20<04:32,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:27,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:23,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:19,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:16,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:13,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:09,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:45<04:06,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<03:59,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:55,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:51,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:02<03:47,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:44,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:10<03:40,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:37,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:34,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:29,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:26,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:27<03:23,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:20,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:35<03:16,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:13,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:09,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:06,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:02,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:52<02:58,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:54,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [05:00<02:51,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:44,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:41,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:38,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:18<02:34,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:30,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:23,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:19,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:35<02:15,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:43<02:08,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:05,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:01,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:53,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:00<01:50,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:08<01:43,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:40,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:33,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:25<01:25,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:33<01:18,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:14,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:43<01:07,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:03,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:50<01:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:56,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:57<00:53,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:08<00:42,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:15<00:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:32,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:23<00:28,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:33<00:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:40<00:10,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:48<00:03,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.058592308312654495, 'eval_runtime': 475.2066, 'eval_samples_per_second': 8.901, 'eval_steps_per_second': 0.28, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m15%|        | 80/528 [2:34:46<11:32:38, 92.76s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 81/528 [2:36:20<29:14:19, 235.48s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 82/528 [2:37:52<23:51:10, 192.53s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 83/528 [2:39:24<20:04:02, 162.34s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 84/528 [2:40:56<17:25:07, 141.23s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 85/528 [2:42:27<15:31:31, 126.17s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 86/528 [2:43:58<14:13:04, 115.80s/it]\u001b[0m\n",
      "\u001b[34m16%|        | 87/528 [2:45:29<13:16:09, 108.32s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 88/528 [2:47:01<12:37:34, 103.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0574, 'grad_norm': 0.603430942655837, 'learning_rate': 6e-06, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m17%|        | 88/528 [2:47:01<12:37:34, 103.31s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 89/528 [2:48:31<12:06:43, 99.32s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 90/528 [2:50:04<11:50:50, 97.38s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 91/528 [2:51:35<11:35:30, 95.49s/it]\u001b[0m\n",
      "\u001b[34m17%|        | 92/528 [2:53:06<11:24:14, 94.16s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 93/528 [2:54:38<11:17:42, 93.48s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 94/528 [2:56:09<11:10:42, 92.72s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 95/528 [2:57:40<11:06:28, 92.35s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 96/528 [2:59:12<11:02:40, 92.04s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0578, 'grad_norm': 0.5946831272343343, 'learning_rate': 6e-06, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m18%|        | 96/528 [2:59:12<11:02:40, 92.04s/it]\u001b[0m\n",
      "\u001b[34m18%|        | 97/528 [3:00:43<10:59:42, 91.84s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 98/528 [3:02:14<10:57:18, 91.72s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 99/528 [3:03:48<10:59:32, 92.24s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 100/528 [3:05:19<10:56:27, 92.03s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 22:08:36,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 22:08:36,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 22:08:36,176 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 22:08:36,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 22:08:36,176 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 22:08:36,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:48,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:25,  2.50s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:15,  2.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:43,  3.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:58,  3.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:07,  3.39s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:25<07:12,  3.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:13,  3.49s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:13,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:11,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:08,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:05,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:04,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:50<07:00,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:56,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:57<06:52,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:49,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:46,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:15<06:37,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:32,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:28,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:24,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:19,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:16,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:40<06:11,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:08,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:03,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<05:59,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:56,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:50,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:05<05:46,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:43,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:36,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:34,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:30,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:30<05:20,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:12,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:08,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:05,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:02,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:55<04:56,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:53,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:49,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:45,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:42,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:38,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:34,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:20<04:30,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:27,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:20,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:16,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:12,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:10,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:45<04:06,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<03:59,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:56,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:52,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:02<03:48,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:45,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:10<03:41,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:38,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:35,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:30,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:26,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:27<03:22,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:19,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:35<03:16,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:13,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:10,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:06,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:02,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:52<02:58,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:54,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [04:59<02:50,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:46,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:43,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:40,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:37,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:17<02:34,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:26,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:23,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:19,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:35<02:15,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:42<02:08,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:04,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:58,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:54,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:00<01:50,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:47,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:07<01:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:33,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:25<01:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:33<01:18,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:15,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:43<01:07,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:50<01:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:57<00:53,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:49,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:08<00:42,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:15<00:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:32,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:22<00:28,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:33<00:17,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:40<00:10,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:47<00:03,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.05680137127637863, 'eval_runtime': 474.9843, 'eval_samples_per_second': 8.906, 'eval_steps_per_second': 0.28, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m19%|        | 100/528 [3:13:14<10:56:27, 92.03s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 101/528 [3:14:46<27:48:50, 234.50s/it]\u001b[0m\n",
      "\u001b[34m19%|        | 102/528 [3:16:17<22:39:47, 191.52s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 103/528 [3:17:49<19:03:57, 161.50s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 104/528 [3:19:22<16:35:17, 140.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0566, 'grad_norm': 0.6062617861201803, 'learning_rate': 6e-06, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m20%|        | 104/528 [3:19:22<16:35:17, 140.84s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 105/528 [3:20:55<14:52:07, 126.54s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 106/528 [3:22:26<13:35:39, 115.97s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 107/528 [3:23:58<12:42:26, 108.66s/it]\u001b[0m\n",
      "\u001b[34m20%|        | 108/528 [3:25:29<12:03:37, 103.38s/it]\u001b[0m\n",
      "\u001b[34m21%|        | 109/528 [3:27:01<11:37:55, 99.94s/it]\u001b[0m\n",
      "\u001b[34m21%|        | 110/528 [3:28:33<11:21:29, 97.82s/it]\u001b[0m\n",
      "\u001b[34m21%|        | 111/528 [3:30:06<11:09:19, 96.30s/it]\u001b[0m\n",
      "\u001b[34m21%|        | 112/528 [3:31:38<10:58:24, 94.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0567, 'grad_norm': 0.466805150744767, 'learning_rate': 6e-06, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m21%|        | 112/528 [3:31:38<10:58:24, 94.96s/it]\u001b[0m\n",
      "\u001b[34m21%|       | 113/528 [3:33:09<10:48:55, 93.82s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 114/528 [3:34:42<10:44:15, 93.37s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 115/528 [3:36:15<10:42:47, 93.38s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 116/528 [3:37:46<10:37:08, 92.79s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 117/528 [3:39:18<10:32:10, 92.29s/it]\u001b[0m\n",
      "\u001b[34m22%|       | 118/528 [3:40:49<10:29:11, 92.08s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 119/528 [3:42:21<10:27:09, 92.00s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 120/528 [3:43:53<10:25:54, 92.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0559, 'grad_norm': 0.44668404694211694, 'learning_rate': 6e-06, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m23%|       | 120/528 [3:43:53<10:25:54, 92.05s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 22:47:09,957 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 22:47:09,957 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 22:47:09,957 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 22:47:09,957 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 22:47:09,957 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 22:47:09,957 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:53,  1.78s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:28,  2.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:15,  2.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:43,  3.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:58,  3.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:04,  3.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:25<07:10,  3.45s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:13,  3.49s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:14,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:13,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:08,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:05,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:02,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:50<06:59,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:57<06:52,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:50,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:47,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:45,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:41,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:15<06:35,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:32,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:22,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:18,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:40<06:10,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:06,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:04,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<06:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:56,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:47,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:04<05:43,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:40,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:39,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:36,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:33,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:30<05:21,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:18,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:09,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:06,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:02,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:54<04:56,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:51,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:47,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:45,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:36,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:19<04:31,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:28,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:20,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:16,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:12,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:08,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:44<04:06,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<03:58,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:53,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:51,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:02<03:48,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:44,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:09<03:41,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:37,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:34,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:31,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:26,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:27<03:24,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:20,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:34<03:17,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:13,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:09,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:06,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:52<02:58,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:55,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [04:59<02:51,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:48,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:44,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:41,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:37,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:17<02:34,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:30,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:23,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:19,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:35<02:16,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:42<02:08,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:04,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:54,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:00<01:50,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:07<01:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:39,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:14<01:36,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:32,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:25<01:25,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:21,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:32<01:18,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:15,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:39<01:11,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:43<01:07,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:50<01:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:57<00:53,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:04<00:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:08<00:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:15<00:35,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:32,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:22<00:28,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:29<00:21,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:33<00:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:40<00:10,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:47<00:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.05502048879861832, 'eval_runtime': 474.9399, 'eval_samples_per_second': 8.906, 'eval_steps_per_second': 0.28, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m23%|       | 120/528 [3:51:48<10:25:54, 92.05s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 121/528 [3:53:20<26:30:20, 234.45s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 122/528 [3:54:52<21:37:12, 191.71s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 123/528 [3:56:25<18:14:11, 162.10s/it]\u001b[0m\n",
      "\u001b[34m23%|       | 124/528 [3:57:58<15:51:25, 141.30s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 125/528 [3:59:31<14:12:29, 126.92s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 126/528 [4:01:04<13:02:47, 116.83s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 127/528 [4:02:36<12:11:14, 109.41s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 128/528 [4:04:08<11:34:37, 104.19s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0545, 'grad_norm': 0.4585188188699984, 'learning_rate': 6e-06, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m24%|       | 128/528 [4:04:08<11:34:37, 104.19s/it]\u001b[0m\n",
      "\u001b[34m24%|       | 129/528 [4:05:39<11:06:24, 100.21s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 130/528 [4:07:12<10:49:59, 97.99s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 131/528 [4:08:44<10:37:18, 96.32s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 132/528 [4:10:15<10:24:12, 94.58s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 133/528 [4:11:46<10:15:24, 93.48s/it]\u001b[0m\n",
      "\u001b[34m25%|       | 134/528 [4:13:17<10:08:29, 92.66s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 135/528 [4:14:48<10:04:33, 92.30s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 136/528 [4:16:21<10:04:44, 92.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0541, 'grad_norm': 0.524103452734955, 'learning_rate': 6e-06, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m26%|       | 136/528 [4:16:21<10:04:44, 92.56s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 137/528 [4:17:52<10:00:31, 92.15s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 138/528 [4:19:24<9:57:46, 91.97s/it]\u001b[0m\n",
      "\u001b[34m26%|       | 139/528 [4:20:57<9:57:58, 92.23s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 140/528 [4:22:29<9:56:17, 92.21s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 23:25:45,906 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-08 23:25:45,906 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 23:25:45,907 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-08 23:25:45,907 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 23:25:45,908 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-08 23:25:45,908 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:50,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:26,  2.51s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:17,  2.93s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:45,  3.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:59,  3.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:04,  3.37s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:25<07:10,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:12,  3.49s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:13,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:12,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:09,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:05,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:02,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:50<06:59,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:57<06:53,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:51,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:15<06:36,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:34,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:30,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:23,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:33<06:20,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:17,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:40<06:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:09,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:06,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:51<06:03,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:58,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:58<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:48,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:05<05:43,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:40,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:38,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:31,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:27,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:30<05:20,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:17,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:06,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:00,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:55<04:56,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:54,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:48,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:44,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:42,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:38,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:20<04:31,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:28,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:24,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:20,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:15,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:12,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:10,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:45<04:07,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:04,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<03:59,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:55,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:52,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:03<03:48,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:44,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:10<03:41,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:38,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:34,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:30,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:26,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:28<03:23,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:20,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:35<03:17,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:12,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:09,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:05,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:02,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:53<02:58,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:55,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [05:00<02:51,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:47,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:44,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:41,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:37,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:18<02:34,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:30,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:27,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:23,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:19,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:36<02:15,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:43<02:08,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:05,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:01,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:57,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:54,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:01<01:50,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:47,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:08<01:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:40,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:33,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:26<01:25,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:33<01:18,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:15,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:44<01:08,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:51<01:00,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:58<00:53,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:50,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:09<00:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:16<00:35,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:32,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:23<00:28,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:34<00:17,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:41<00:10,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:48<00:03,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:52<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.053739458322525024, 'eval_runtime': 475.6647, 'eval_samples_per_second': 8.893, 'eval_steps_per_second': 0.28, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m27%|       | 140/528 [4:30:25<9:56:17, 92.21s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:52<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 141/528 [4:31:57<25:15:11, 234.91s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 142/528 [4:33:28<20:34:15, 191.85s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 143/528 [4:35:01<17:20:22, 162.14s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 144/528 [4:36:33<15:02:18, 140.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0535, 'grad_norm': 0.45307916376519913, 'learning_rate': 6e-06, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m27%|       | 144/528 [4:36:33<15:02:18, 140.99s/it]\u001b[0m\n",
      "\u001b[34m27%|       | 145/528 [4:38:04<13:24:28, 126.03s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 146/528 [4:39:37<12:19:22, 116.13s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 147/528 [4:41:10<11:33:38, 109.23s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 148/528 [4:42:42<10:58:25, 103.96s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 149/528 [4:44:13<10:32:46, 100.18s/it]\u001b[0m\n",
      "\u001b[34m28%|       | 150/528 [4:45:46<10:16:57, 97.93s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 151/528 [4:47:17<10:02:18, 95.86s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 152/528 [4:48:48<9:52:01, 94.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0521, 'grad_norm': 0.44969266688365445, 'learning_rate': 6e-06, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m29%|       | 152/528 [4:48:48<9:52:01, 94.47s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 153/528 [4:50:19<9:43:08, 93.30s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 154/528 [4:51:51<9:39:46, 93.01s/it]\u001b[0m\n",
      "\u001b[34m29%|       | 155/528 [4:53:23<9:36:44, 92.77s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 156/528 [4:54:55<9:32:51, 92.40s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 157/528 [4:56:26<9:30:05, 92.20s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 158/528 [4:57:58<9:27:26, 92.02s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 159/528 [4:59:30<9:25:31, 91.96s/it]\u001b[0m\n",
      "\u001b[34m30%|       | 160/528 [5:01:03<9:26:14, 92.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0522, 'grad_norm': 0.38707857057510886, 'learning_rate': 6e-06, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m30%|       | 160/528 [5:01:03<9:26:14, 92.32s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-09 00:04:19,847 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-09 00:04:19,847 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-09 00:04:19,852 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-09 00:04:19,852 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-09 00:04:19,853 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-09 00:04:19,853 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:55,  1.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:30,  2.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:19,  2.94s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:44,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:59,  3.30s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:06,  3.38s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:25<07:10,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:10,  3.47s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:12,  3.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:11,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:10,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:06,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:03,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:50<06:59,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:56,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:57<06:52,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:50,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:47,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:41,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:15<06:37,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:33,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:25,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:19,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:15,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:40<06:11,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:07,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:02,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<05:59,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:56,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:05<05:45,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:41,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:39,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:35,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:31,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:28,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:30<05:21,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:16,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:14,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:09,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:05,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:01,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<04:59,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:54<04:55,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:53,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:49,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:42,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:37,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:34,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:19<04:31,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:27,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:23,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:19,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:16,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:13,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:11,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:45<04:08,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:03,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<04:00,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:56,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:52,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:02<03:49,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:45,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:10<03:42,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:38,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:34,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:30,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:27,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:27<03:23,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:19,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:35<03:16,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:13,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:10,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:06,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:02,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:52<02:58,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:55,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [05:00<02:50,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:44,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:40,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:37,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:18<02:34,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:30,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:23,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:19,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:35<02:16,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:43<02:09,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:05,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:01,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:58,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:54,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:00<01:50,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:47,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:08<01:43,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:40,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:33,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:26<01:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:33<01:18,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:15,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:44<01:08,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:51<01:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:58<00:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:09<00:43,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:16<00:35,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:32,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:23<00:28,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:34<00:17,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:41<00:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:48<00:03,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:52<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.05241132155060768, 'eval_runtime': 475.6604, 'eval_samples_per_second': 8.893, 'eval_steps_per_second': 0.28, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m30%|       | 160/528 [5:08:59<9:26:14, 92.32s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:52<00:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 161/528 [5:10:31<23:56:49, 234.90s/it]\u001b[0m\n",
      "\u001b[34m31%|       | 162/528 [5:12:02<19:29:51, 191.78s/it]\u001b[0m\n",
      "\u001b[34m31%|       | 163/528 [5:13:34<16:24:37, 161.86s/it]\u001b[0m\n",
      "\u001b[34m31%|       | 164/528 [5:15:06<14:14:58, 140.93s/it]\u001b[0m\n",
      "\u001b[34m31%|      | 165/528 [5:16:39<12:46:16, 126.66s/it]\u001b[0m\n",
      "\u001b[34m31%|      | 166/528 [5:18:12<11:43:04, 116.53s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 167/528 [5:19:45<10:59:04, 109.54s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 168/528 [5:21:19<10:28:40, 104.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0523, 'grad_norm': 0.381676179484994, 'learning_rate': 6e-06, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m32%|      | 168/528 [5:21:19<10:28:40, 104.78s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 169/528 [5:22:52<10:05:42, 101.23s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 170/528 [5:24:24<9:48:10, 98.58s/it]\u001b[0m\n",
      "\u001b[34m32%|      | 171/528 [5:25:56<9:34:10, 96.50s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 172/528 [5:27:28<9:24:38, 95.17s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 173/528 [5:29:00<9:17:41, 94.26s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 174/528 [5:30:33<9:13:20, 93.79s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 175/528 [5:32:04<9:07:08, 93.00s/it]\u001b[0m\n",
      "\u001b[34m33%|      | 176/528 [5:33:35<9:02:16, 92.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0511, 'grad_norm': 0.42962800394317163, 'learning_rate': 6e-06, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m33%|      | 176/528 [5:33:35<9:02:16, 92.43s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 177/528 [5:35:07<8:59:17, 92.19s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 178/528 [5:36:38<8:56:34, 91.99s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 179/528 [5:38:10<8:54:14, 91.85s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 180/528 [5:39:41<8:51:04, 91.56s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-09 00:42:57,580 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-04-09 00:42:57,580 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-09 00:42:57,583 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-04-09 00:42:57,583 >>   Num examples = 4230\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-09 00:42:57,583 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-04-09 00:42:57,583 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/133 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 2/133 [00:03<03:54,  1.79s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|         | 3/133 [00:07<05:28,  2.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|         | 4/133 [00:10<06:16,  2.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|         | 5/133 [00:14<06:43,  3.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 6/133 [00:17<06:55,  3.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|         | 7/133 [00:21<07:02,  3.36s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|         | 8/133 [00:24<07:07,  3.42s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|         | 9/133 [00:28<07:09,  3.46s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 10/133 [00:32<07:12,  3.52s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|         | 11/133 [00:35<07:10,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|         | 12/133 [00:39<07:07,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|         | 13/133 [00:42<07:04,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|         | 14/133 [00:46<07:02,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|        | 15/133 [00:49<07:00,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|        | 16/133 [00:53<06:58,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|        | 17/133 [00:57<06:54,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 18/133 [01:00<06:50,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|        | 19/133 [01:04<06:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|        | 20/133 [01:07<06:42,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|        | 21/133 [01:11<06:40,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 22/133 [01:15<06:38,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|        | 23/133 [01:18<06:33,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|        | 24/133 [01:22<06:29,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|        | 25/133 [01:25<06:26,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 26/133 [01:29<06:20,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|        | 27/133 [01:32<06:20,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|        | 28/133 [01:36<06:15,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|       | 29/133 [01:40<06:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 30/133 [01:43<06:08,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|       | 31/133 [01:47<06:04,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|       | 32/133 [01:50<06:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|       | 33/133 [01:54<05:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 34/133 [01:57<05:53,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|       | 35/133 [02:01<05:49,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|       | 36/133 [02:05<05:46,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|       | 37/133 [02:08<05:43,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 38/133 [02:12<05:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|       | 39/133 [02:15<05:36,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|       | 40/133 [02:19<05:33,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|       | 41/133 [02:22<05:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 42/133 [02:26<05:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|      | 43/133 [02:30<05:22,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|      | 44/133 [02:33<05:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 45/133 [02:37<05:13,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 46/133 [02:40<05:10,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|      | 47/133 [02:44<05:06,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|      | 48/133 [02:47<05:03,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|      | 49/133 [02:51<05:00,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 50/133 [02:55<04:56,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|      | 51/133 [02:58<04:53,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|      | 52/133 [03:02<04:50,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|      | 53/133 [03:05<04:47,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|      | 54/133 [03:09<04:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|     | 55/133 [03:12<04:39,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|     | 56/133 [03:16<04:35,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|     | 57/133 [03:20<04:31,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 58/133 [03:23<04:28,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|     | 59/133 [03:27<04:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|     | 60/133 [03:30<04:20,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|     | 61/133 [03:34<04:16,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 62/133 [03:37<04:12,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|     | 63/133 [03:41<04:08,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|     | 64/133 [03:45<04:06,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|     | 65/133 [03:48<04:03,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 66/133 [03:52<03:58,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|     | 67/133 [03:55<03:55,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|     | 68/133 [03:59<03:51,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|    | 69/133 [04:02<03:48,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 70/133 [04:06<03:45,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|    | 71/133 [04:10<03:41,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|    | 72/133 [04:13<03:37,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|    | 73/133 [04:17<03:34,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 74/133 [04:20<03:30,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|    | 75/133 [04:24<03:25,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|    | 76/133 [04:27<03:22,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|    | 77/133 [04:31<03:19,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 78/133 [04:35<03:16,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|    | 79/133 [04:38<03:12,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|    | 80/133 [04:42<03:09,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|    | 81/133 [04:45<03:05,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 82/133 [04:49<03:02,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|   | 83/133 [04:52<02:58,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|   | 84/133 [04:56<02:54,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|   | 85/133 [05:00<02:50,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 86/133 [05:03<02:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|   | 87/133 [05:07<02:44,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|   | 88/133 [05:10<02:42,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|   | 89/133 [05:14<02:38,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 90/133 [05:18<02:35,  3.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|   | 91/133 [05:21<02:31,  3.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|   | 92/133 [05:25<02:27,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|   | 93/133 [05:28<02:24,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|   | 94/133 [05:32<02:20,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|  | 95/133 [05:36<02:15,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|  | 96/133 [05:39<02:12,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|  | 97/133 [05:43<02:08,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 98/133 [05:46<02:04,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|  | 99/133 [05:50<02:00,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|  | 100/133 [05:53<01:56,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|  | 101/133 [05:57<01:53,  3.55s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 102/133 [06:00<01:50,  3.56s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|  | 103/133 [06:04<01:47,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|  | 104/133 [06:08<01:43,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|  | 105/133 [06:11<01:39,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 106/133 [06:15<01:36,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|  | 107/133 [06:18<01:33,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|  | 108/133 [06:22<01:29,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%| | 109/133 [06:25<01:25,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 110/133 [06:29<01:22,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%| | 111/133 [06:33<01:18,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%| | 112/133 [06:36<01:15,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%| | 113/133 [06:40<01:11,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 114/133 [06:43<01:07,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%| | 115/133 [06:47<01:04,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%| | 116/133 [06:50<01:00,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%| | 117/133 [06:54<00:57,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 118/133 [06:58<00:53,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%| | 119/133 [07:01<00:50,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%| | 120/133 [07:05<00:46,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%| | 121/133 [07:08<00:43,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 122/133 [07:12<00:39,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|| 123/133 [07:16<00:35,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|| 124/133 [07:19<00:32,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|| 125/133 [07:23<00:28,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 126/133 [07:26<00:24,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|| 127/133 [07:30<00:21,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|| 128/133 [07:33<00:17,  3.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|| 129/133 [07:37<00:14,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 130/133 [07:41<00:10,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|| 131/133 [07:44<00:07,  3.58s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|| 132/133 [07:48<00:03,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:51<00:00,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.05135020613670349, 'eval_runtime': 475.495, 'eval_samples_per_second': 8.896, 'eval_steps_per_second': 0.28, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m34%|      | 180/528 [5:47:36<8:51:04, 91.56s/it]\u001b[0m\n",
      "\u001b[34m100%|| 133/133 [07:52<00:00,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m34%|      | 181/528 [5:49:06<22:31:48, 233.74s/it]\u001b[0m\n",
      "\u001b[34m34%|      | 182/528 [5:50:37<18:20:11, 190.79s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 183/528 [5:52:09<15:26:30, 161.13s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 184/528 [5:53:40<13:23:39, 140.17s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0513, 'grad_norm': 0.3974873114606033, 'learning_rate': 6e-06, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m35%|      | 184/528 [5:53:40<13:23:39, 140.17s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 185/528 [5:55:11<11:56:47, 125.39s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 186/528 [5:56:42<10:56:53, 115.24s/it]\u001b[0m\n",
      "\u001b[34m35%|      | 187/528 [5:58:15<10:16:17, 108.44s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 188/528 [5:59:47<9:46:19, 103.47s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 189/528 [6:01:18<9:23:48, 99.79s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 190/528 [6:02:50<9:09:39, 97.57s/it]\u001b[0m\n",
      "\u001b[34m36%|      | 191/528 [6:04:22<8:58:27, 95.87s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "output_s3_path = f\"s3://{sess.default_bucket()}/model/email-names/{model}/{timestamp}\"\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True, \n",
    "    instance_type = \"ml.g5.24xlarge\",\n",
    "    output_path=output_s3_path,\n",
    "    metric_definitions=[{'Name': 'train:loss', 'Regex': \"'loss': ([0-9]+\\.[0-9]+)\"}]\n",
    ")\n",
    "\n",
    "# we can set the hyperparameters below:\n",
    "estimator.set_hyperparameters(epoch=\"2\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89ab83-b510-46ef-9751-17415302427d",
   "metadata": {},
   "source": [
    "#### Training Job Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6328111f-22d0-4f1c-92cc-5a11a4ee99e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.6660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>780.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2700.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3420.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4680.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5400.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6600.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7320.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8100.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.0594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp metric_name   value\n",
       "0        0.0  train:loss  0.6660\n",
       "1      780.0  train:loss  0.0825\n",
       "2     1980.0  train:loss  0.0730\n",
       "3     2700.0  train:loss  0.0927\n",
       "4     3420.0  train:loss  0.0704\n",
       "5     4680.0  train:loss  0.0658\n",
       "6     5400.0  train:loss  0.0625\n",
       "7     6600.0  train:loss  0.0609\n",
       "8     7320.0  train:loss  0.0607\n",
       "9     8100.0  train:loss  0.0594"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "training_job_name = estimator.latest_training_job.job_name\n",
    "\n",
    "training_analytics_df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "training_analytics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2daf56-648e-4bfa-814f-fc09d5ddd67d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Copying model files to folder 'model-artifacts' (for deployment purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75110cd4-fcde-4b8b-8dc6-391a7436f32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "#source_path can either be the s3 output location where the model artifacts are stored which is output_s3_path in this notebook, or it can be hard-coded\n",
    "#source_path = output_s3_path \n",
    "source_path = 's3://sagemaker-sigparser-caylent-mlops/model/email-names/Mistral-7B/2024-04-01_06-04-42/hf-llm-mistral-7b-2024-04-01-06-20-01-214/'\n",
    "source_parts = source_path[5:].split('/', 1)\n",
    "source_bucket = source_parts[0]\n",
    "source_prefix = source_parts[1]\n",
    "destination_prefix = 'model-artifacts/'+ source_prefix.split('model/email-names/')[1]\n",
    "\n",
    "folders_to_copy = ['debug-output', 'profiler-output', 'output']\n",
    "for folder in folders_to_copy:\n",
    "    #List all objects in the source directory containing all three output folders after training\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for result in paginator.paginate(Bucket=source_bucket, Prefix=source_prefix + folder):\n",
    "        if 'Contents' in result:\n",
    "            for obj in result['Contents']:\n",
    "                key = obj['Key']\n",
    "                new_key = key.replace(source_prefix, destination_prefix, 1)\n",
    "                s3_client.copy_object(\n",
    "                    Bucket=source_bucket,\n",
    "                    CopySource={'Bucket': source_bucket, 'Key': key},\n",
    "                    Key=new_key\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3512ff0-56bb-4e7c-a3da-13f479a00e48",
   "metadata": {},
   "source": [
    "### Deploy Trained Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "838e79de-f96d-4a41-ac45-04c9d2b46cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:Using model 'huggingface-llm-mistral-7b' with version '2.3.0'. You can upgrade to version '2.3.1' to get the latest model specifications. Note that models may have different input/output signatures after a major version upgrade.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: hf-llm-mistral-7b-2024-04-09-14-04-34-438\n",
      "INFO:sagemaker:Creating endpoint-config with name hf-llm-mistral-7b-2024-04-09-14-04-34-436\n",
      "INFO:sagemaker:Creating endpoint with name hf-llm-mistral-7b-2024-04-09-14-04-34-436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85b8f1f8-c903-43d7-8bae-e825d41d0123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\nFirst Name: Akshay\\nMiddle Name: \\nLast Name: Bodnar\\nName Prefix: \\nName Suffix: \\n\\n \\nFirst Name: Akshay\\nMiddle Name: \\nLast Name: Bodnar\\nName Prefix: \\nName Suffix: \\n\\n \\nFirst Name: Akshay\\nMiddle Name: \\nLast Name: Bodnar\\nName Prefix: \\nName Suffix'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test finetuned model\n",
    "\n",
    "input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "context = contexts[1]\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": template[\"prompt\"].format(\n",
    "        system_prompt=system_prompt, instruction=instruction, context=context\n",
    "    )\n",
    "    + input_output_demarkation_key,\n",
    "    \"parameters\": {\"max_new_tokens\": 100, \"temperature\":0.1, 'top_p':0.1},\n",
    "}\n",
    "\n",
    "finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "\n",
    "finetuned_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f937a7f4-1141-44a5-be54-3cbc36fe9081",
   "metadata": {},
   "source": [
    "### Incremental training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "844cf2a4-2529-4820-aba3-8fb64f9686e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import Estimator\n",
    "\n",
    "incremental_train_data_location='s3://sagemaker-sigparser-caylent-mlops/data/email-names/input/training/Mistral-7B/2024-05-01_10-31-25/mistral-7b-incremental-training-dataset-sp_llm_emailname_training-apr8.jsonl'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") \n",
    "model = 'Mistral-7B'\n",
    "output_s3_path = f\"s3://{sess.default_bucket()}/model/email-names/{model}/{timestamp}\"\n",
    "last_training_job = 'hf-llm-mistral-7b-2024-04-08-18-50-53-623'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde39ae3-2982-43d8-91de-5f0588cee540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "#Retrieve all details about the previous training job. Several items will be picked up from the resulting JSON object to create the estimator in the code block below.\n",
    "training_job_info = sagemaker_client.describe_training_job(TrainingJobName=last_training_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4104b97-9a60-4902-8fa3-0601c6a8b7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: mistral-incremental-training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-01 18:09:05 Starting - Starting the training job...\n",
      "2024-05-01 18:09:33 Pending - Training job waiting for capacity...\n",
      "2024-05-01 18:09:57 Pending - Preparing the instances for training......\n",
      "2024-05-01 18:10:50 Downloading - Downloading input data.....................\n",
      "2024-05-01 18:14:26 Downloading - Downloading the training image...\n",
      "2024-05-01 18:14:56 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:07,406 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:07,459 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:07,468 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:07,470 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:11,839 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.26.1-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/deepspeed/deepspeed-0.10.3.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.15-py3-none-any.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/flash_attn/flash_attn-2.5.5-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ninja/ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/packaging/packaging-23.2-py3-none-any.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.8.2-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py_cpuinfo/py_cpuinfo-9.0.0-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/rich/rich-13.7.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.6.5-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.38.1-py3-none-any.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.7.10-py3-none-any.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.2-py3-none-any.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.2-py2.py3-none-any.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.10-py2.py3-none-any.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.26.1->-r requirements.txt (line 1)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 3)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.5.5->-r requirements.txt (line 5)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich==13.7.0->-r requirements.txt (line 10)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich==13.7.0->-r requirements.txt (line 10)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 14)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 14)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1->-r requirements.txt (line 14)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.7.10->-r requirements.txt (line 15)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.2->-r requirements.txt (line 16)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.26.1->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich==13.7.0->-r requirements.txt (line 10)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.10->-r requirements.txt (line 15)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 14)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10->-r requirements.txt (line 15)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.10->-r requirements.txt (line 15)) (1.16.0)\u001b[0m\n",
      "\u001b[34mninja is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mpy-cpuinfo is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.10.3-py3-none-any.whl size=907844 sha256=a1af2d640a155c08a1e7ad6e26f60658f21745634878ff8c513e878e1d6354d1\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b4/a0/a9/4723ccba9b5790d90f40617f369a69c6dff729fa4b0aa6e131\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: shtab, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, packaging, docstring-parser, rich, bitsandbytes, tyro, tokenizers, flash-attn, deepspeed, accelerate, transformers, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: packaging\u001b[0m\n",
      "\u001b[34mFound existing installation: packaging 23.1\u001b[0m\n",
      "\u001b[34mUninstalling packaging-23.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled packaging-23.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: rich\u001b[0m\n",
      "\u001b[34mFound existing installation: rich 13.4.2\u001b[0m\n",
      "\u001b[34mUninstalling rich-13.4.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled rich-13.4.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.26.1 bitsandbytes-0.42.0 deepspeed-0.10.3 docstring-parser-0.15 flash-attn-2.5.5 packaging-23.2 peft-0.8.2 rich-13.7.0 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.2 sagemaker-jumpstart-script-utilities-1.1.10 sagemaker-jumpstart-tabular-script-utilities-1.0.0 shtab-1.6.5 tokenizers-0.15.1 transformers-4.38.1 trl-0.7.10 tyro-0.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:40,182 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:40,182 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:40,284 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:40,377 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:40,471 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:40,481 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epoch\": \"1\",\n",
      "        \"max_input_length\": \"1024\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mistral-incremental-training\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/llm/prepack/v1.1.2/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epoch\":\"1\",\"max_input_length\":\"1024\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/llm/prepack/v1.1.2/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epoch\":\"1\",\"max_input_length\":\"1024\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mistral-incremental-training\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/llm/prepack/v1.1.2/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epoch\",\"1\",\"--max_input_length\",\"1024\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --epoch 1 --max_input_length 1024\u001b[0m\n",
      "\u001b[34m2024-05-01 18:16:40,511 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mFound existing installation: diffusers 0.16.1\u001b[0m\n",
      "\u001b[34mUninstalling diffusers-0.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled diffusers-0.16.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:44,048] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mINFO:root:Running training scripts with arguments: Namespace(model_dir='/opt/ml/model', train='/opt/ml/input/data/training', train_alt=None, validation=None, hosts=['algo-1'], num_gpus=4, current_host='algo-1', pretrained_model='/opt/ml/input/data/model', peft_type='None', lora_r=64, lora_alpha=16, lora_dropout=0.0, bits=16, double_quant=True, quant_type='nf4', deepspeed=True, instruction_tuned='True', chat_dataset='False', train_from_scratch='False', fp16='True', bf16='False', evaluation_strategy='steps', eval_steps=20, epoch=1, gradient_accumulation_steps=1, per_device_train_batch_size=4, per_device_eval_batch_size=8, logging_steps=20, warmup_ratio=0.1, learning_rate=5e-06, weight_decay=0.1, load_best_model_at_end='True', max_train_samples=-1, max_val_samples=-1, seed=10, max_input_length=1024, validation_split_ratio=0.05, train_data_split_seed=0, preprocessing_num_workers=None, max_steps=-1, gradient_checkpointing='False', early_stopping_patience=3, early_stopping_threshold=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, label_smoothing_factor=0, logging_strategy='steps', logging_first_step='False', logging_nan_inf_filter='True', save_strategy='steps', save_steps=500, save_total_limit=1, dataloader_drop_last='False', dataloader_num_workers=0, eval_accumulation_steps=None, auto_find_batch_size='False', lr_scheduler_type='linear', warmup_steps=0, add_input_output_demarcation_key=True).\u001b[0m\n",
      "\u001b[34mINFO:root:Ignoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mINFO:root:Using uncompressed artifacts in /opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mINFO:root:Parameter 'instruction_tuned' is 'True'. Starting instruction fine-tuning.\u001b[0m\n",
      "\u001b[34mINFO:root:Running command ['deepspeed', '--num_gpus=4', '/opt/conda/lib/python3.10/site-packages/sagemaker_jumpstart_huggingface_script_utilities/fine_tuning/run_clm.py', '--deepspeed', 'ds_config.json', '--model_name_or_path', '/opt/ml/input/data/model', '--train_file', '/opt/ml/input/data/training', '--do_train', '--output_dir', '/opt/ml/model', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '8', '--logging_steps', '20', '--warmup_ratio', '0.1', '--learning_rate', '5e-06', '--weight_decay', '0.1', '--seed', '10', '--max_input_length', '1024', '--validation_split_ratio', '0.05', '--train_data_split_seed', '0', '--max_steps', '-1', '--early_stopping_patience', '3', '--early_stopping_threshold', '0.01', '--adam_beta1', '0.9', '--adam_beta2', '0.999', '--max_grad_norm', '1.0', '--label_smoothing_factor', '0', '--logging_strategy', 'steps', '--save_strategy', 'steps', '--save_steps', '500', '--dataloader_num_workers', '0', '--lr_scheduler_type', 'linear', '--warmup_steps', '0', '--evaluation_strategy', 'steps', '--eval_steps', '20', '--lora_r', '64', '--lora_alpha', '16', '--lora_dropout', '0.0', '--bits', '16', '--quant_type', 'nf4', '--add_input_output_demarcation_key', 'True', '--load_best_model_at_end', '--fp16', '--instruction_tuned', '--save_total_limit', '1', '--double_quant']\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:49,201] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:51,966] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:51,966] [INFO] [runner.py:570:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /opt/conda/lib/python3.10/site-packages/sagemaker_jumpstart_huggingface_script_utilities/fine_tuning/run_clm.py --deepspeed ds_config.json --model_name_or_path /opt/ml/input/data/model --train_file /opt/ml/input/data/training --do_train --output_dir /opt/ml/model --num_train_epochs 1 --gradient_accumulation_steps 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --logging_steps 20 --warmup_ratio 0.1 --learning_rate 5e-06 --weight_decay 0.1 --seed 10 --max_input_length 1024 --validation_split_ratio 0.05 --train_data_split_seed 0 --max_steps -1 --early_stopping_patience 3 --early_stopping_threshold 0.01 --adam_beta1 0.9 --adam_beta2 0.999 --max_grad_norm 1.0 --label_smoothing_factor 0 --logging_strategy steps --save_strategy steps --save_steps 500 --dataloader_num_workers 0 --lr_scheduler_type linear --warmup_steps 0 --evaluation_strategy steps --eval_steps 20 --lora_r 64 --lora_alpha 16 --lora_dropout 0.0 --bits 16 --quant_type nf4 --add_input_output_demarcation_key True --load_best_model_at_end --fp16 --instruction_tuned --save_total_limit 1 --double_quant\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:53,544] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,226] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,226] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,226] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,226] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,226] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,226] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,227] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,227] [INFO] [launch.py:163:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:16:56,227] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:02,084] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:02,087] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:02,089] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:02,094] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:05,298] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:05,314] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:05,315] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:05,315] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:05,367] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:06 - WARNING - jumpstart -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:06 - WARNING - jumpstart -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:06 - WARNING - jumpstart -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:06 - WARNING - jumpstart -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:06 - INFO - jumpstart -   Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34maccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_persistent_workers=False,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataloader_prefetch_factor=None,\u001b[0m\n",
      "\u001b[34mddp_backend=None,\u001b[0m\n",
      "\u001b[34mddp_broadcast_buffers=None,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=ds_config.json,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdispatch_batches=None,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=20,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=steps,\u001b[0m\n",
      "\u001b[34mfp16=True,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing_kwargs=None,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_always_push=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34minclude_num_input_tokens_seen=False,\u001b[0m\n",
      "\u001b[34minclude_tokens_per_second=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-06,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=0,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=warning,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/May01_18-17-05_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=20,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_kwargs={},\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mneftune_noise_alpha=None,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_torch,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_only_model=False,\u001b[0m\n",
      "\u001b[34msave_safetensors=True,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=1,\u001b[0m\n",
      "\u001b[34mseed=10,\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msplit_batches=None,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_cpu=False,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.1,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.1,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2044] 2024-05-01 18:17:06,560 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:314] 2024-05-01 18:17:06,610 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:314] 2024-05-01 18:17:06,610 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:726] 2024-05-01 18:17:06,613 >> loading configuration file /opt/ml/input/data/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:726] 2024-05-01 18:17:06,613 >> loading configuration file /opt/ml/input/data/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:791] 2024-05-01 18:17:06,614 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32004\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:791] 2024-05-01 18:17:06,614 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32004\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:06 - INFO - jumpstart -   Overwrite use_cache to be False in the model config.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3254] 2024-05-01 18:17:06,663 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3254] 2024-05-01 18:17:06,663 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1400] 2024-05-01 18:17:06,664 >> Instantiating MistralForCausalLM model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1400] 2024-05-01 18:17:06,664 >> Instantiating MistralForCausalLM model under default dtype torch.float16.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3363] 2024-05-01 18:17:06,664 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3363] 2024-05-01 18:17:06,664 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:329] 2024-05-01 18:17:06,667 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:329] 2024-05-01 18:17:06,667 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-05-01 18:17:06,675 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-05-01 18:17:06,675 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mNCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:13,043] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 7.24B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:03<00:07,  3.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:03<00:07,  3.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:03<00:07,  3.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|      | 1/3 [00:03<00:07,  3.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.61s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 10 examples [00:00, 3321.17 examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerate examples in the format of prompt template: 100%|| 10/10 [00:00<00:00, 2369.53 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 3/3 [00:10<00:00,  3.64s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3992] 2024-05-01 18:17:23,998 >> All model checkpoint weights were used when initializing MistralForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3992] 2024-05-01 18:17:23,998 >> All model checkpoint weights were used when initializing MistralForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4000] 2024-05-01 18:17:23,998 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4000] 2024-05-01 18:17:23,998 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/10 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:798] 2024-05-01 18:17:24,001 >> loading configuration file /opt/ml/input/data/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:798] 2024-05-01 18:17:24,001 >> loading configuration file /opt/ml/input/data/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-05-01 18:17:24,002 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:845] 2024-05-01 18:17:24,002 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|| 10/10 [00:00<00:00, 1054.14 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|| 10/10 [00:00<00:00, 931.43 examples/s]\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:24 - INFO - jumpstart -   Training data is identified. The corresponded column names are ['system_prompt', 'instruction', 'context', 'response'].\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:24 - INFO - jumpstart -   The max sequence length is set as 1024.\u001b[0m\n",
      "\u001b[34m05/01/2024 18:17:24 - INFO - jumpstart -   Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1875] 2024-05-01 18:17:24,108 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32004. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1875] 2024-05-01 18:17:24,108 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32004. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:601] 2024-05-01 18:17:24,557 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:601] 2024-05-01 18:17:24,557 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:24,685] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:24,695] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:25,829] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:25,829] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:25,830] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:25,835] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/4] /opt/conda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.3018856048584 seconds\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.307517528533936 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.31342387199402 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.403959274291992 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000005, betas=(0.900000, 0.999000), weight_decay=0.100000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,145] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,161] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,161] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,161] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,161] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,277] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,277] [INFO] [utils.py:804:see_memory_usage] MA 0.99 GB         Max_MA 1.48 GB         CA 1.07 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,278] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 48.06 GB, percent = 12.9%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,279] [INFO] [stage3.py:126:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,279] [INFO] [stage3.py:127:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,395] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,396] [INFO] [utils.py:804:see_memory_usage] MA 0.99 GB         Max_MA 0.99 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,396] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 48.06 GB, percent = 12.9%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,736] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,737] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.99 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,737] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 48.56 GB, percent = 13.0%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,856] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,857] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:17:57,857] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 48.56 GB, percent = 13.0%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:00,573] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:00,575] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:00,575] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 60.94 GB, percent = 16.3%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:00,793] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:00,794] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:00,795] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 64.56 GB, percent = 17.3%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:02,231] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:02,232] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:02,232] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 80.86 GB, percent = 21.6%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:02,422] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:02,423] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:02,423] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 87.18 GB, percent = 23.3%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:07,135] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:07,136] [INFO] [utils.py:804:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 1.07 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:07,136] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 162.02 GB, percent = 43.4%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:07,136] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,720] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,721] [INFO] [utils.py:804:see_memory_usage] MA 0.53 GB         Max_MA 1.02 GB         CA 1.31 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,721] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 175.62 GB, percent = 47.0%\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f13ec03fac0>\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-06], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,722] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,722] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f13fabc5990>\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,723] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 4096\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   optimizer_params ............. {'lr': 5e-06, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.1}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-06, 'warmup_num_steps': 1}\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   train_batch_size ............. 16\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  4\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,724] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:971:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:971:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:971:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:10,725] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 5e-06, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.1\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 5e-06, \n",
      "            \"warmup_num_steps\": 1\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1812] 2024-05-01 18:18:10,725 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1813] 2024-05-01 18:18:10,725 >>   Num examples = 9\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1814] 2024-05-01 18:18:10,725 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1815] 2024-05-01 18:18:10,725 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1818] 2024-05-01 18:18:10,725 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1819] 2024-05-01 18:18:10,725 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1820] 2024-05-01 18:18:10,725 >>   Total optimization steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1812] 2024-05-01 18:18:10,725 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1813] 2024-05-01 18:18:10,725 >>   Num examples = 9\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1814] 2024-05-01 18:18:10,725 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1815] 2024-05-01 18:18:10,725 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1818] 2024-05-01 18:18:10,725 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1819] 2024-05-01 18:18:10,725 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1820] 2024-05-01 18:18:10,725 >>   Total optimization steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1821] 2024-05-01 18:18:10,726 >>   Number of trainable parameters = 7,241,764,864\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1821] 2024-05-01 18:18:10,726 >>   Number of trainable parameters = 7,241,764,864\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:23<00:00, 23.10s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2067] 2024-05-01 18:18:33,828 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2067] 2024-05-01 18:18:33,828 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 23.1016, 'train_samples_per_second': 0.39, 'train_steps_per_second': 0.043, 'train_loss': 1.5270919799804688, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:23<00:00, 23.10s/it]\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:23<00:00, 23.11s/it]\u001b[0m\n",
      "\u001b[34m***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.5271\n",
      "  train_runtime            = 0:00:23.10\n",
      "  train_samples            =          9\n",
      "  train_samples_per_second =       0.39\n",
      "  train_steps_per_second   =      0.043\u001b[0m\n",
      "\u001b[34m05/01/2024 18:18:33 - INFO - jumpstart -   Start Evaluation.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-05-01 18:18:33,850 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3376] 2024-05-01 18:18:33,850 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-05-01 18:18:33,850 >>   Num examples = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-05-01 18:18:33,850 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3378] 2024-05-01 18:18:33,850 >>   Num examples = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3381] 2024-05-01 18:18:33,850 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|| 1/1 [00:00<00:00, 685.57it/s]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\u001b[0m\n",
      "\u001b[34mepoch                   =        1.0\n",
      "  eval_loss               =     0.5484\n",
      "  eval_runtime            = 0:00:02.98\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =      0.335\n",
      "  eval_steps_per_second   =      0.335\n",
      "  perplexity              =     1.7304\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3067] 2024-05-01 18:18:49,524 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3067] 2024-05-01 18:18:49,524 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:473] 2024-05-01 18:18:49,525 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:473] 2024-05-01 18:18:49,525 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:614] 2024-05-01 18:18:49,525 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:614] 2024-05-01 18:18:49,525 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:53,352] [INFO] [launch.py:347:main] Process 151 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:54,354] [INFO] [launch.py:347:main] Process 152 exits successfully.\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:18:54,354] [INFO] [launch.py:347:main] Process 153 exits successfully.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2462] 2024-05-01 18:19:03,346 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /opt/ml/model/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2462] 2024-05-01 18:19:03,346 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /opt/ml/model/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2459] 2024-05-01 18:19:03,346 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2459] 2024-05-01 18:19:03,346 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2468] 2024-05-01 18:19:03,347 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2468] 2024-05-01 18:19:03,347 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2024-05-01 18:19:07,367] [INFO] [launch.py:347:main] Process 150 exits successfully.\u001b[0m\n",
      "\u001b[34m2024-05-01 18:19:10,002 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-05-01 18:19:10,002 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-05-01 18:19:10,002 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-05-01 18:19:12 Uploading - Uploading generated training model\n",
      "2024-05-01 18:19:48 Completed - Training job completed\n",
      "Training seconds: 538\n",
      "Billable seconds: 538\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    role=role,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True, \n",
    "    instance_type = \"ml.g5.24xlarge\",\n",
    "    instance_count=1,\n",
    "    image_uri=training_job_info['AlgorithmSpecification']['TrainingImage'], #Previous training job's image uri\n",
    "    model_uri=training_job_info['ModelArtifacts']['S3ModelArtifacts'], #Trained model's S3 uri (tar.gz file)\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    source_dir=training_job_info['InputDataConfig'][2]['DataSource']['S3DataSource']['S3Uri'], #Source directory containing transfer_learning.py\n",
    "    output_path=output_s3_path,\n",
    "    metric_definitions=[{'Name': 'train:loss', 'Regex': \"'loss': ([0-9]+\\.[0-9]+)\"}]\n",
    ")\n",
    "\n",
    "# we can set the hyperparameters below:\n",
    "estimator.set_hyperparameters(epoch=\"1\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": incremental_train_data_location}, logs=True, job_name='mistral-incremental-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab06970-68ed-4393-8fd5-05707d8ebe0d",
   "metadata": {},
   "source": [
    "#### The model artifacts for the above training job are stored in 's3://sagemaker-sigparser-caylent-mlops/model/email-names/Mistral-7B/2024-05-01_18-08-55/mistral-incremental-training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bdca6-e45c-4cbf-adb1-b2de69f9304b",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3506d26d-3d8e-4c95-94a4-222fd3cb993d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete the SageMaker endpoint\n",
    "\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e3da7-2730-450a-ae1a-2c061c9e48c8",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Miscellaneous Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ea61a-fb03-45c3-a968-50ef4b6c8434",
   "metadata": {},
   "source": [
    "### Default Instance Types\n",
    "\n",
    "Review sagemaker default instance type given the model id and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20421027-325f-4701-ab55-980812f9b087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml.g5.2xlarge\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import instance_types\n",
    "\n",
    "# check default instance type\n",
    "instance_type = instance_types.retrieve_default(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    scope=\"inference\")\n",
    "print(instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157c39d-f678-483a-be8d-dc1c854de809",
   "metadata": {},
   "source": [
    "### Recommended Instance Types\n",
    "\n",
    "Review sagemaker recommended instance types for inference, given the model id and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb4f809f-267e-4bd3-9cb9-1d19c70f1571",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ml.g5.2xlarge', 'ml.g5.4xlarge', 'ml.g5.8xlarge', 'ml.g5.16xlarge']\n"
     ]
    }
   ],
   "source": [
    "# check all the recommended instance types for inference\n",
    "instance_type = instance_types.retrieve(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    scope=\"inference\")\n",
    "print(instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577369f-a583-4f00-aaca-398ddfc3dfe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy Uncompressed Model\n",
    "\n",
    "We can deploy the uncompressed model, which can be used for inferencing. In order to deploy the uncompressed finetuned model, we need the following:\n",
    "- Inference Image\n",
    "- Model\n",
    "- Endpoint Config\n",
    "- Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3527e80a-45ad-481a-b3c8-b3b7f8f75cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-llm-mistral-7b\", \"2.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282da1b9-cb63-4e51-a57f-546560591ced",
   "metadata": {},
   "source": [
    "#### Inference Image\n",
    "\n",
    "Fetch the inference image based on the model id and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "789e0d41-0619-4a7d-9362-6e0cad74c4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_image_uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "# Must specify JumpStart `model_id` and `model_version` when getting specs for JumpStart models.\n",
    "from sagemaker import image_uris\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    ")\n",
    "print(\"inference_image_uri: {}\".format(inference_image_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2d09a-2611-49bc-8573-d19469fbc2c3",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Create model using the inference image and the model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877e5230-74eb-4c85-a26c-4b3aaf9c14ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model using the artifacts data\n",
    "model_name = 'mistral-7b-email-name-finetuned'\n",
    "sagemaker_role = role\n",
    "container = inference_image_uri\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = sagemaker_role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": container,\n",
    "        \"ModelDataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3Uri\": \"s3://sagemaker-sigparser-caylent-mlops/model/email-names/Mistral-7B/2024-04-08_18-48-25/hf-llm-mistral-7b-2024-04-08-18-50-53-623/output/model/\", \n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\",\n",
    "                \"ModelAccessConfig\": {\n",
    "                    \"AcceptEula\": True\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"Environment\": {\n",
    "            'HF_MODEL_ID': '/opt/ml/model'\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b7fccda-fbfd-462f-ac40-d4a6a13d58dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:818442660361:model/mistral-7b-email-name-finetuned', 'ResponseMetadata': {'RequestId': '9d06df70-ea90-4531-8bb3-f01c0cdc034e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9d06df70-ea90-4531-8bb3-f01c0cdc034e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '93', 'date': 'Tue, 16 Apr 2024 21:58:27 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d179dea1-a385-4f88-b677-44bc1c53ca86",
   "metadata": {},
   "source": [
    "#### Endpoint Config\n",
    "\n",
    "Create the endpoint config: specify the instance count, type, model name, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "facb1c70-fae6-42af-b7cb-fa1340db4100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = 'mistral-7b-email-name-finetuned'\n",
    "endpoint_config_name = f'{endpoint_name}-config'\n",
    "\n",
    "create_endpoint_config_api_response = sagemaker_client.create_endpoint_config(\n",
    "                                            EndpointConfigName=endpoint_config_name,\n",
    "                                            ProductionVariants=[\n",
    "                                                {\n",
    "                                                    'VariantName': 'dev',\n",
    "                                                    'ModelName': model_name,\n",
    "                                                    'InitialInstanceCount': 1,\n",
    "                                                    'InstanceType': 'ml.g5.2xlarge'\n",
    "                                                },\n",
    "                                            ]\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54c159-8764-4a7b-b6b1-ec720861baac",
   "metadata": {},
   "source": [
    "#### Endpoint\n",
    "\n",
    "Create the model endpoint by specifying the endpoint name and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61c7c2fe-2541-4be0-9a44-a57a0d3a9495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_api_response = sagemaker_client.create_endpoint(\n",
    "                                    EndpointName=endpoint_name,\n",
    "                                    EndpointConfigName=endpoint_config_name,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9dc4368-92a7-4e3a-a342-5bfe8bcebe89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_endpoint API response: {'EndpointArn': 'arn:aws:sagemaker:us-east-1:818442660361:endpoint/mistral-7b-email-name-finetuned', 'ResponseMetadata': {'RequestId': '4de0ecba-2049-4569-8373-b11b40ad6b06', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '4de0ecba-2049-4569-8373-b11b40ad6b06', 'content-type': 'application/x-amz-json-1.1', 'content-length': '99', 'date': 'Tue, 16 Apr 2024 21:58:32 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(f'create_endpoint API response: {create_endpoint_api_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9db219-273a-42fd-93d4-82f8c0c54447",
   "metadata": {},
   "source": [
    "#### Cleanup\n",
    "\n",
    "Delete the deployed endpoint and the related resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58219b52-c7d3-4d48-8d93-7e1c481e9cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '30e979c4-efa2-440e-9ac7-c3dddccc517c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '30e979c4-efa2-440e-9ac7-c3dddccc517c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Tue, 16 Apr 2024 21:58:12 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete model\n",
    "sagemaker_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730dbb6-d91b-4f02-9e88-cec340ab9211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
