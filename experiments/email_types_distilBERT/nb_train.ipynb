{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267f9bde-a95f-4ffd-a471-f2d082d8b097",
   "metadata": {},
   "source": [
    "### This end-to-end training notebook contains code to fine-tune the Multilingual Cased DistilBERT model with or without hyperparameter optimization on our training dataset and create a model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab61a0d-c731-422a-9291-09575e346f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the utility notebook first\n",
    "%run utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f5756",
   "metadata": {},
   "source": [
    "### Setup where to store/access data files\n",
    "- Access all training data files: https://us-east-1.console.aws.amazon.com/s3/buckets/sigparser-models?region=us-east-1&bucketType=general&prefix=email-types-distilBERT/&showversions=false\n",
    "- IMPORTANT: Make sure this naming convention is used (e.g. **data-train-email-types-yyyy-mm-dd-a.csv**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547225e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The INPUT will change every time we run this notebook with a new dataset\n",
    "INPUT = \"data-train-email-types-yyyy-mm-dd-a.csv\"\n",
    "OUTPUT = INPUT.removesuffix(\".csv\")\n",
    "\n",
    "print(f\"INPUT: {INPUT}\")\n",
    "print(f\"OUTPUT: {OUTPUT}\")\n",
    "\n",
    "#These values won't change unless we change the structure of our S3 bucket\n",
    "S3_BUCKET_NAME = 'sigparser-models'\n",
    "S3_FOLDER_NAME = 'email-types-distilBERT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cd5d8",
   "metadata": {},
   "source": [
    "### Clean the raw training data (IMPORTANT: Make sure you uploaded the training data on the S3 bucket in the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c279f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = retrieve_from_s3(bucket_name=S3_BUCKET_NAME, file_key=f\"{S3_FOLDER_NAME}/{INPUT}\")\n",
    "print(f\"Number of rows in the training dataset: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ac2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_TYPE_COL = \"Email Type\"\n",
    "EMAIL_ADDRESS_COL = \"Email Address\"\n",
    "EMAIL_NAME_COL = \"Email Name\"\n",
    "EMAIL_DISPLAY_NAME_COL = \"Email Display Name\"\n",
    "\n",
    "print(\"Shape of train dataframe: \", train_df.shape)\n",
    "print(\"Value count of Email Type values in train dataset: \", train_df[EMAIL_TYPE_COL].value_counts())\n",
    "\n",
    "nan_values(train_df)\n",
    "\n",
    "#Convert Email Type str values to numeric values\n",
    "train_df[EMAIL_TYPE_COL] = train_df[EMAIL_TYPE_COL].apply(email_type_to_int)\n",
    "\n",
    "#Concatenating the email columns\n",
    "train_df['combined'] = train_df[EMAIL_ADDRESS_COL]+ ', ' +train_df[EMAIL_NAME_COL]+ ', ' +train_df[EMAIL_DISPLAY_NAME_COL]\n",
    "\n",
    "#Picking out only the Email Type and combined string columns to be in the final train df\n",
    "final_df = train_df[[EMAIL_TYPE_COL, 'combined']]\n",
    "\n",
    "#Saving train data csv file to S3 \n",
    "save_to_s3(final_df, bucket_name=S3_BUCKET_NAME, file_key=f\"{S3_FOLDER_NAME}/{OUTPUT}/data.csv\", mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9cdb4-ed3e-4a44-b21b-c286f850bb03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Starting a SageMaker session with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f3aeba7-ceea-4126-b430-7158da51466b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484bbb4-7d00-4512-8eb0-460fd29c9c26",
   "metadata": {},
   "source": [
    "### Setting Model ID and version for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e9b0903-4035-4fe8-bfc4-d8b56a7835e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = 'huggingface-tc-distilbert-base-multilingual-cased', \"2.0.0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd65bc6-014c-4c11-acae-7909fdc9022c",
   "metadata": {},
   "source": [
    "### Set training data and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9487f8b-fa2e-4e34-a750-32774e1506de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset_s3_path = f\"s3://{S3_BUCKET_NAME}/{S3_FOLDER_NAME}/{OUTPUT}/data.csv\"\n",
    "s3_output_location = f\"s3://{S3_BUCKET_NAME}/{S3_FOLDER_NAME}/{OUTPUT}\"\n",
    "\n",
    "print(f\"Training dataset S3 path: {training_dataset_s3_path}\")\n",
    "print(f\"S3 output location: {s3_output_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64190fab-18e1-444d-956f-9afeb9c526cd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "048f4524-2096-47bc-94e8-43c978e73b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': '4',\n",
    " 'learning_rate': '2e-05',\n",
    " 'batch_size': '64',\n",
    " 'eval_batch_size': '8',\n",
    " 'eval_accumulation_steps': 'None',\n",
    " 'reinitialize_top_layer': 'Auto',\n",
    " 'train_only_top_layer': 'False'}\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    output_path=s3_output_location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cca66cf5-0cff-40d3-9144-2e36a88522f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: distilbert-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using estimator\n",
      "2024-04-26 21:49:33 Starting - Starting the training job...\n",
      "2024-04-26 21:49:33 Pending - Training job waiting for capacity............\n",
      "2024-04-26 21:51:36 Pending - Preparing the instances for training...\n",
      "2024-04-26 21:52:14 Downloading - Downloading input data...\n",
      "2024-04-26 21:52:44 Downloading - Downloading the training image........................\n",
      "2024-04-26 21:56:51 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-26 21:57:20,767 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-26 21:57:20,794 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:57:20,797 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:57:20,805 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker-jumpstart-prepack-script-utilities\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-jumpstart-prepack-script-utilities-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: There was an error checking the latest version of pip.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:57:23,076 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:57:23,076 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:57:23,179 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": \"64\",\n",
      "        \"epochs\": \"4\",\n",
      "        \"eval_accumulation_steps\": \"None\",\n",
      "        \"eval_batch_size\": \"8\",\n",
      "        \"learning_rate\": \"2e-05\",\n",
      "        \"reinitialize_top_layer\": \"Auto\",\n",
      "        \"train_only_top_layer\": \"False\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"distilbert-model\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":\"64\",\"epochs\":\"4\",\"eval_accumulation_steps\":\"None\",\"eval_batch_size\":\"8\",\"learning_rate\":\"2e-05\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"False\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":\"64\",\"epochs\":\"4\",\"eval_accumulation_steps\":\"None\",\"eval_batch_size\":\"8\",\"learning_rate\":\"2e-05\",\"reinitialize_top_layer\":\"Auto\",\"train_only_top_layer\":\"False\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"distilbert-model\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--epochs\",\"4\",\"--eval_accumulation_steps\",\"None\",\"--eval_batch_size\",\"8\",\"--learning_rate\",\"2e-05\",\"--reinitialize_top_layer\",\"Auto\",\"--train_only_top_layer\",\"False\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_ACCUMULATION_STEPS=None\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_REINITIALIZE_TOP_LAYER=Auto\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_ONLY_TOP_LAYER=False\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --batch_size 64 --epochs 4 --eval_accumulation_steps None --eval_batch_size 8 --learning_rate 2e-05 --reinitialize_top_layer Auto --train_only_top_layer False\u001b[0m\n",
      "\u001b[34mRunning training scripts with arguments: Namespace(batch_size=64, current_host='algo-1', epochs=4, eval_accumulation_steps='None', eval_batch_size=8, hosts=['algo-1'], hub_key=None, learning_rate=2e-05, logging_frequency=50, metric_for_best_model='f1', model_dir='/opt/ml/model', pretrained_model='/opt/ml/input/data/model', reinitialize_top_layer='Auto', seed=0, train='/opt/ml/input/data/training', train_only_top_layer='False').\u001b[0m\n",
      "\u001b[34mIgnoring unrecognized arguments: [].\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-c82403fe43286b5f\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c82403fe43286b5f/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 6472.69it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 1295.34it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c82403fe43286b5f/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 873.63it/s]\u001b[0m\n",
      "\u001b[34mloaded dataset sizes is: 3397\u001b[0m\n",
      "\u001b[34mThe labels distribution in the dataset is: {1: 2355, 0: 1042}\u001b[0m\n",
      "\u001b[34mParameter 'function'=<function _prepare_data.<locals>.<lambda> at 0x7f86e8070040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m235ex [00:00, 2344.58ex/s]\u001b[0m\n",
      "\u001b[34m470ex [00:00, 2069.37ex/s]\u001b[0m\n",
      "\u001b[34m843ex [00:00, 2774.55ex/s]\u001b[0m\n",
      "\u001b[34m1182ex [00:00, 3004.79ex/s]\u001b[0m\n",
      "\u001b[34m1611ex [00:00, 3453.92ex/s]\u001b[0m\n",
      "\u001b[34m2003ex [00:00, 3606.40ex/s]\u001b[0m\n",
      "\u001b[34m2402ex [00:00, 3729.24ex/s]\u001b[0m\n",
      "\u001b[34m2826ex [00:00, 3888.00ex/s]\u001b[0m\n",
      "\u001b[34m3217ex [00:00, 3663.44ex/s]\u001b[0m\n",
      "\u001b[34m3397ex [00:00, 3408.52ex/s]\u001b[0m\n",
      "\u001b[34mloaded train_dataset sizes is: 2717\u001b[0m\n",
      "\u001b[34mThe labels distribution in the train dataset is: {1: 1876, 0: 841}\u001b[0m\n",
      "\u001b[34mloaded eval_dataset sizes is: 680\u001b[0m\n",
      "\u001b[34mThe labels distribution in the eval dataset is: {1: 479, 0: 201}\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 2717\u001b[0m\n",
      "\u001b[34mNum examples = 2717\u001b[0m\n",
      "\u001b[34mNum Epochs = 4\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 172\u001b[0m\n",
      "\u001b[34mNum Epochs = 4\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 172\u001b[0m\n",
      "\u001b[34m[2024-04-26 21:57:40.894 algo-1:46 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-04-26 21:57:41.059 algo-1:46 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-04-26 21:57:41.060 algo-1:46 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-04-26 21:57:41.061 algo-1:46 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-04-26 21:57:41.062 algo-1:46 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-04-26 21:57:41.062 algo-1:46 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m{'loss': 0.6621, 'learning_rate': 1.988372093023256e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\u001b[0m\n",
      "\u001b[34mNum examples = 680\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.13454914093017578, 'eval_accuracy': 0.9558823529411765, 'eval_f1': 0.956508437917983, 'eval_precision': 0.9593712680095148, 'eval_recall': 0.9558823529411765, 'eval_runtime': 0.9095, 'eval_samples_per_second': 747.649, 'eval_steps_per_second': 93.456, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-43\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-43\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-43/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-43/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-43/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-43/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-43/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-43/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-43/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-43/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.3074, 'learning_rate': 1.4186046511627909e-05, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.0799405574798584, 'eval_accuracy': 0.975, 'eval_f1': 0.9750533255768857, 'eval_precision': 0.975151384083045, 'eval_recall': 0.975, 'eval_runtime': 0.974, 'eval_samples_per_second': 698.184, 'eval_steps_per_second': 87.273, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-86\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-86\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-86/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-86/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-86/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-86/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-86/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-86/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-86/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-86/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-43] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-43] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0753, 'learning_rate': 8.372093023255815e-06, 'epoch': 2.33}\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.07876692712306976, 'eval_accuracy': 0.9764705882352941, 'eval_f1': 0.976436274509804, 'eval_precision': 0.976422162174453, 'eval_recall': 0.9764705882352941, 'eval_runtime': 0.9399, 'eval_samples_per_second': 723.498, 'eval_steps_per_second': 90.437, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-129\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-129\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-129/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-129/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-129/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-129/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-129/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-129/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-129/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-129/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-86] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34mDeleting older checkpoint [/tmp/checkpoint-86] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.032, 'learning_rate': 2.558139534883721e-06, 'epoch': 3.49}\u001b[0m\n",
      "\n",
      "2024-04-26 21:58:36 Uploading - Uploading generated training model\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentences. If sentences are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 680\u001b[0m\n",
      "\u001b[34mNum examples = 680\n",
      "  Batch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.08996591717004776, 'eval_accuracy': 0.9735294117647059, 'eval_f1': 0.9737110439604132, 'eval_precision': 0.9743797342095926, 'eval_recall': 0.9735294117647059, 'eval_runtime': 0.9114, 'eval_samples_per_second': 746.14, 'eval_steps_per_second': 93.267, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-172\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/checkpoint-172\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-172/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/checkpoint-172/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-172/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/checkpoint-172/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-172/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/checkpoint-172/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-172/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/checkpoint-172/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-129 (score: 0.976436274509804).\u001b[0m\n",
      "\u001b[34mLoading best model from /tmp/checkpoint-129 (score: 0.976436274509804).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 48.0963, 'train_samples_per_second': 225.963, 'train_steps_per_second': 3.576, 'train_loss': 0.1259420167568118, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mInfo file not found at '/tmp/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:58:30,978 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:58:30,978 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-26 21:58:30,979 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-04-26 21:58:57 Completed - Training job completed\n",
      "Training seconds: 403\n",
      "Billable seconds: 403\n"
     ]
    }
   ],
   "source": [
    "#For Hyperparameter Optimization, set use_amt to True\n",
    "use_amt = False \n",
    "\n",
    "#For HPO:Select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"val_accuracy\", \"Regex\": \"'eval_accuracy': ([0-9\\\\.]+)\"}],\n",
    "    \"type\": \"Maximize\",\n",
    "}\n",
    "\n",
    "#If use_amt is set to True, a hyperparameter optimization job is carried out.\n",
    "if use_amt:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        estimator, \n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=6,\n",
    "        max_parallel_jobs=2,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=\"distilbert-multilingual\",  \n",
    "    )\n",
    "\n",
    "    #Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"training\": training_dataset_s3_path})\n",
    "    \n",
    "#Else, a regular training job is created\n",
    "else:\n",
    "    #Note that the training job's name must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "    print(\"using estimator\")\n",
    "    estimator.fit({\"training\": training_dataset_s3_path}, logs=True, job_name=f\"{OUTPUT}-model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0fbdd307-6e2b-4718-9dcd-7ead5eccb570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>hugginface-tc:eval-accuracy</td>\n",
       "      <td>0.955882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.0</td>\n",
       "      <td>hugginface-tc:eval-accuracy</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                  metric_name     value\n",
       "0        0.0  hugginface-tc:eval-accuracy  0.955882\n",
       "1       60.0  hugginface-tc:eval-accuracy  0.975000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training performance metrics\n",
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "    model_data = hp_tuner.best_estimator().model_data\n",
    "else:\n",
    "    training_job_name = estimator.latest_training_job.job_name\n",
    "    model_data = estimator.model_data\n",
    "    \n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010bcf1-b001-42d3-8852-9e8c8d24adf5",
   "metadata": {},
   "source": [
    "### Refactoring compressed model artifacts and re-routing S3 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "592ebc43-6a8f-420c-9a29-c43e14740b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and Uploaded config.json to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/config.json\n",
      "Extracted and Uploaded tokenizer.json to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/tokenizer.json\n",
      "Extracted and Uploaded pytorch_model.bin to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/pytorch_model.bin\n",
      "Extracted and Uploaded tokenizer_config.json to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/tokenizer_config.json\n",
      "Extracted and Uploaded vocab.txt to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/vocab.txt\n",
      "Extracted and Uploaded special_tokens_map.json to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/special_tokens_map.json\n",
      "Extracted and Uploaded code/inference.py to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/code/inference.py\n",
      "Extracted and Uploaded code/__script_info__.json to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/code/__script_info__.json\n",
      "Extracted and Uploaded code/__init__.py to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/code/__init__.py\n",
      "Extracted and Uploaded code/constants/__init__.py to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/code/constants/__init__.py\n",
      "Extracted and Uploaded code/constants/constants.py to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/code/constants/constants.py\n",
      "Extracted and Uploaded code/version to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/code/version\n",
      "Extracted and Uploaded __models_info__.json to sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model/__models_info__.json\n"
     ]
    }
   ],
   "source": [
    "#The model artifacts are in tar.gz format. The files specifically in subfolder 'output' are extracted and stored in 'output/model'\n",
    "source_bucket = model_data[5:].split('/', 1)[0]\n",
    "source_key = model_data[5:].split('/', 1)[1]\n",
    "\n",
    "#Destination path where the tar.gz file contents should be extracted and stored.\n",
    "dest_bucket = 'sagemaker-sigparser-caylent-mlops'\n",
    "dest_key_prefix = source_key[:source_key.rfind('.tar.gz')]+'/'\n",
    "\n",
    "obj = s3_client.get_object(Bucket=source_bucket, Key=source_key)\n",
    "file_stream = BytesIO(obj['Body'].read())\n",
    "\n",
    "with tarfile.open(fileobj=file_stream, mode='r:gz') as tar:\n",
    "    for file in tar.getmembers():\n",
    "        if file.isfile():\n",
    "            with tar.extractfile(file) as ind_file:\n",
    "                dest_key = f\"{dest_key_prefix}{file.name}\"\n",
    "                #upload_fileobj() below will stream each file's content directly to S3. This avoids loading the full content into memory.\n",
    "                s3_client.upload_fileobj(ind_file, Bucket=dest_bucket, Key=dest_key)\n",
    "                print(f\"Extracted and Uploaded {file.name} to {dest_bucket}/{dest_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "298fae7f-4b88-41ed-ad36-4e18a39771af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleted the original tar.gz model file from sagemaker-sigparser-caylent-mlops/model-artifacts/distilBERT/2024-04-26_21-33-18/distilbert-model/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#Deleting the tar.gz file\n",
    "s3_client.delete_object(Bucket=source_bucket, Key=source_key)\n",
    "print(f'\\nDeleted the original tar.gz model file from {source_bucket}/{source_key}')"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
